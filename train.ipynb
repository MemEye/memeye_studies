{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Dataset\n",
    "import torch\n",
    "from model import LSTMModel\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Training Accuracy:  0.34116083296411165\n",
      "Epoch:  0 Val Accuracy:  0.24185724185724186\n",
      "Epoch:  1 Training Accuracy:  0.36896322552060257\n",
      "Epoch:  1 Val Accuracy:  0.2674982674982675\n",
      "Epoch:  2 Training Accuracy:  0.36984935755427556\n",
      "Epoch:  2 Val Accuracy:  0.2758142758142758\n",
      "Epoch:  3 Training Accuracy:  0.3975409836065574\n",
      "Epoch:  3 Val Accuracy:  0.31046431046431044\n",
      "Epoch:  4 Training Accuracy:  0.37660611431103236\n",
      "Epoch:  4 Val Accuracy:  0.2273042273042273\n",
      "Epoch:  5 Training Accuracy:  0.39499335400974744\n",
      "Epoch:  5 Val Accuracy:  0.358974358974359\n",
      "Epoch:  6 Training Accuracy:  0.4212450155073106\n",
      "Epoch:  6 Val Accuracy:  0.36936936936936937\n",
      "Epoch:  7 Training Accuracy:  0.4077315019937971\n",
      "Epoch:  7 Val Accuracy:  0.36174636174636177\n",
      "Epoch:  8 Training Accuracy:  0.43198936641559593\n",
      "Epoch:  8 Val Accuracy:  0.3970893970893971\n",
      "Epoch:  9 Training Accuracy:  0.45159503766061143\n",
      "Epoch:  9 Val Accuracy:  0.27927927927927926\n",
      "Epoch:  10 Training Accuracy:  0.48305272485600353\n",
      "Epoch:  10 Val Accuracy:  0.33471933471933474\n",
      "Epoch:  11 Training Accuracy:  0.509636685866194\n",
      "Epoch:  11 Val Accuracy:  0.39154539154539153\n",
      "Epoch:  12 Training Accuracy:  0.5183872396987151\n",
      "Epoch:  12 Val Accuracy:  0.3839223839223839\n",
      "Epoch:  13 Training Accuracy:  0.5425343376163049\n",
      "Epoch:  13 Val Accuracy:  0.43451143451143454\n",
      "Epoch:  14 Training Accuracy:  0.5477403633141338\n",
      "Epoch:  14 Val Accuracy:  0.38253638253638256\n",
      "Epoch:  15 Training Accuracy:  0.5664599025254763\n",
      "Epoch:  15 Val Accuracy:  0.46846846846846846\n",
      "Epoch:  16 Training Accuracy:  0.5952591936198494\n",
      "Epoch:  16 Val Accuracy:  0.4851004851004851\n",
      "Epoch:  17 Training Accuracy:  0.6123172352680549\n",
      "Epoch:  17 Val Accuracy:  0.4941094941094941\n",
      "Epoch:  18 Training Accuracy:  0.6164155959237927\n",
      "Epoch:  18 Val Accuracy:  0.5980595980595981\n",
      "Epoch:  19 Training Accuracy:  0.6345813026140895\n",
      "Epoch:  19 Val Accuracy:  0.5343035343035343\n",
      "Epoch:  20 Training Accuracy:  0.6644882587505538\n",
      "Epoch:  20 Val Accuracy:  0.6299376299376299\n",
      "Epoch:  21 Training Accuracy:  0.6714665485157288\n",
      "Epoch:  21 Val Accuracy:  0.6417186417186417\n",
      "Epoch:  22 Training Accuracy:  0.687306158617634\n",
      "Epoch:  22 Val Accuracy:  0.6278586278586279\n",
      "Epoch:  23 Training Accuracy:  0.6974966770048737\n",
      "Epoch:  23 Val Accuracy:  0.6008316008316008\n",
      "Epoch:  24 Training Accuracy:  0.6883030571555162\n",
      "Epoch:  24 Val Accuracy:  0.6493416493416494\n",
      "Epoch:  25 Training Accuracy:  0.6940629153743908\n",
      "Epoch:  25 Val Accuracy:  0.6493416493416494\n",
      "Epoch:  26 Training Accuracy:  0.7079087284005317\n",
      "Epoch:  26 Val Accuracy:  0.6521136521136521\n",
      "Epoch:  27 Training Accuracy:  0.7127824545857333\n",
      "Epoch:  27 Val Accuracy:  0.5876645876645876\n",
      "Epoch:  28 Training Accuracy:  0.7126716880815241\n",
      "Epoch:  28 Val Accuracy:  0.6396396396396397\n",
      "Epoch:  29 Training Accuracy:  0.7138901196278246\n",
      "Epoch:  29 Val Accuracy:  0.6382536382536382\n",
      "Epoch:  30 Training Accuracy:  0.7231945059813912\n",
      "Epoch:  30 Val Accuracy:  0.6327096327096328\n",
      "Epoch:  31 Training Accuracy:  0.7327204253433761\n",
      "Epoch:  31 Val Accuracy:  0.6396396396396397\n",
      "Epoch:  32 Training Accuracy:  0.7285112981834293\n",
      "Epoch:  32 Val Accuracy:  0.6597366597366597\n",
      "Epoch:  33 Training Accuracy:  0.7224191404519273\n",
      "Epoch:  33 Val Accuracy:  0.6638946638946639\n",
      "Epoch:  34 Training Accuracy:  0.7293974302171023\n",
      "Epoch:  34 Val Accuracy:  0.6534996534996536\n",
      "Epoch:  35 Training Accuracy:  0.7357111209570226\n",
      "Epoch:  35 Val Accuracy:  0.6410256410256411\n",
      "Epoch:  36 Training Accuracy:  0.7279574656623837\n",
      "Epoch:  36 Val Accuracy:  0.6583506583506583\n",
      "Epoch:  37 Training Accuracy:  0.7307266282676119\n",
      "Epoch:  37 Val Accuracy:  0.6417186417186417\n",
      "Epoch:  38 Training Accuracy:  0.7468985378821444\n",
      "Epoch:  38 Val Accuracy:  0.6361746361746362\n",
      "Epoch:  39 Training Accuracy:  0.752658396101019\n",
      "Epoch:  39 Val Accuracy:  0.6410256410256411\n",
      "Epoch:  40 Training Accuracy:  0.7662826761187417\n",
      "Epoch:  40 Val Accuracy:  0.6375606375606375\n",
      "Epoch:  41 Training Accuracy:  0.7672795746566239\n",
      "Epoch:  41 Val Accuracy:  0.6334026334026334\n",
      "Epoch:  42 Training Accuracy:  0.7727071333628711\n",
      "Epoch:  42 Val Accuracy:  0.6299376299376299\n",
      "Epoch:  43 Training Accuracy:  0.7672795746566239\n",
      "Epoch:  43 Val Accuracy:  0.6237006237006237\n",
      "Epoch:  44 Training Accuracy:  0.7764731945059814\n",
      "Epoch:  44 Val Accuracy:  0.6382536382536382\n",
      "Epoch:  45 Training Accuracy:  0.7744793974302171\n",
      "Epoch:  45 Val Accuracy:  0.6202356202356203\n",
      "Epoch:  46 Training Accuracy:  0.7781346920691183\n",
      "Epoch:  46 Val Accuracy:  0.6313236313236313\n",
      "Epoch:  47 Training Accuracy:  0.7772485600354453\n",
      "Epoch:  47 Val Accuracy:  0.6243936243936244\n",
      "Epoch:  48 Training Accuracy:  0.7766947275143996\n",
      "Epoch:  48 Val Accuracy:  0.6451836451836452\n",
      "Epoch:  49 Training Accuracy:  0.7821222862206468\n",
      "Epoch:  49 Val Accuracy:  0.6216216216216216\n",
      "Epoch:  50 Training Accuracy:  0.7779131590607\n",
      "Epoch:  50 Val Accuracy:  0.6202356202356203\n",
      "Epoch:  51 Training Accuracy:  0.7817899867080195\n",
      "Epoch:  51 Val Accuracy:  0.6306306306306306\n",
      "Epoch:  52 Training Accuracy:  0.7889898094816128\n",
      "Epoch:  52 Val Accuracy:  0.623007623007623\n",
      "Epoch:  53 Training Accuracy:  0.7884359769605671\n",
      "Epoch:  53 Val Accuracy:  0.6271656271656272\n",
      "Epoch:  54 Training Accuracy:  0.7919805050952592\n",
      "Epoch:  54 Val Accuracy:  0.6347886347886348\n",
      "Epoch:  55 Training Accuracy:  0.7920912715994683\n",
      "Epoch:  55 Val Accuracy:  0.638946638946639\n",
      "Epoch:  56 Training Accuracy:  0.7834514842711564\n",
      "Epoch:  56 Val Accuracy:  0.632016632016632\n",
      "Epoch:  0 Training Accuracy:  0.3511298183429331\n",
      "Epoch:  0 Val Accuracy:  0.2785862785862786\n",
      "Epoch:  1 Training Accuracy:  0.3059370846256092\n",
      "Epoch:  1 Val Accuracy:  0.14899514899514898\n",
      "Epoch:  2 Training Accuracy:  0.33617634027470095\n",
      "Epoch:  2 Val Accuracy:  0.2841302841302841\n",
      "Epoch:  3 Training Accuracy:  0.3818121400088613\n",
      "Epoch:  3 Val Accuracy:  0.30561330561330563\n",
      "Epoch:  4 Training Accuracy:  0.39011962782454584\n",
      "Epoch:  4 Val Accuracy:  0.32363132363132363\n",
      "Epoch:  5 Training Accuracy:  0.4229065130704475\n",
      "Epoch:  5 Val Accuracy:  0.358974358974359\n",
      "Epoch:  6 Training Accuracy:  0.4180327868852459\n",
      "Epoch:  6 Val Accuracy:  0.2945252945252945\n",
      "Epoch:  7 Training Accuracy:  0.49966770048737263\n",
      "Epoch:  7 Val Accuracy:  0.3652113652113652\n",
      "Epoch:  8 Training Accuracy:  0.5177226406734603\n",
      "Epoch:  8 Val Accuracy:  0.501039501039501\n",
      "Epoch:  9 Training Accuracy:  0.5640230394328755\n",
      "Epoch:  9 Val Accuracy:  0.4407484407484408\n",
      "Epoch:  10 Training Accuracy:  0.5725520602569782\n",
      "Epoch:  10 Val Accuracy:  0.4941094941094941\n",
      "Epoch:  11 Training Accuracy:  0.6144217988480284\n",
      "Epoch:  11 Val Accuracy:  0.5592515592515592\n",
      "Epoch:  12 Training Accuracy:  0.636796632698272\n",
      "Epoch:  12 Val Accuracy:  0.49133749133749133\n",
      "Epoch:  13 Training Accuracy:  0.621067789100576\n",
      "Epoch:  13 Val Accuracy:  0.5065835065835066\n",
      "Epoch:  14 Training Accuracy:  0.6650420912715995\n",
      "Epoch:  14 Val Accuracy:  0.6514206514206514\n",
      "Epoch:  15 Training Accuracy:  0.6319229065130705\n",
      "Epoch:  15 Val Accuracy:  0.4774774774774775\n",
      "Epoch:  16 Training Accuracy:  0.6659282233052725\n",
      "Epoch:  16 Val Accuracy:  0.6070686070686071\n",
      "Epoch:  17 Training Accuracy:  0.6423349579087284\n",
      "Epoch:  17 Val Accuracy:  0.6181566181566182\n",
      "Epoch:  18 Training Accuracy:  0.6735711120957023\n",
      "Epoch:  18 Val Accuracy:  0.589050589050589\n",
      "Epoch:  19 Training Accuracy:  0.6591714665485158\n",
      "Epoch:  19 Val Accuracy:  0.6022176022176022\n",
      "Epoch:  20 Training Accuracy:  0.7342711564023039\n",
      "Epoch:  20 Val Accuracy:  0.6694386694386695\n",
      "Epoch:  21 Training Accuracy:  0.7221976074435091\n",
      "Epoch:  21 Val Accuracy:  0.665973665973666\n",
      "Epoch:  22 Training Accuracy:  0.7483385024368631\n",
      "Epoch:  22 Val Accuracy:  0.6632016632016632\n",
      "Epoch:  23 Training Accuracy:  0.7392556490917147\n",
      "Epoch:  23 Val Accuracy:  0.6306306306306306\n",
      "Epoch:  24 Training Accuracy:  0.7556490917146654\n",
      "Epoch:  24 Val Accuracy:  0.6729036729036729\n",
      "Epoch:  25 Training Accuracy:  0.7632919805050953\n",
      "Epoch:  25 Val Accuracy:  0.6507276507276507\n",
      "Epoch:  26 Training Accuracy:  0.7703810367744794\n",
      "Epoch:  26 Val Accuracy:  0.6611226611226612\n",
      "Epoch:  27 Training Accuracy:  0.7738147984049624\n",
      "Epoch:  27 Val Accuracy:  0.6604296604296604\n",
      "Epoch:  28 Training Accuracy:  0.7751439964554718\n",
      "Epoch:  28 Val Accuracy:  0.6742896742896742\n",
      "Epoch:  29 Training Accuracy:  0.795303500221533\n",
      "Epoch:  29 Val Accuracy:  0.6597366597366597\n",
      "Epoch:  30 Training Accuracy:  0.8039432875498449\n",
      "Epoch:  30 Val Accuracy:  0.6555786555786556\n",
      "Epoch:  31 Training Accuracy:  0.8149091714665485\n",
      "Epoch:  31 Val Accuracy:  0.6465696465696466\n",
      "Epoch:  32 Training Accuracy:  0.801395657953035\n",
      "Epoch:  32 Val Accuracy:  0.6160776160776161\n",
      "Epoch:  33 Training Accuracy:  0.8105892778023925\n",
      "Epoch:  33 Val Accuracy:  0.6396396396396397\n",
      "Epoch:  34 Training Accuracy:  0.8201151971643775\n",
      "Epoch:  34 Val Accuracy:  0.647955647955648\n",
      "Epoch:  35 Training Accuracy:  0.8132476739034116\n",
      "Epoch:  35 Val Accuracy:  0.665973665973666\n",
      "Epoch:  36 Training Accuracy:  0.8269827204253434\n",
      "Epoch:  36 Val Accuracy:  0.6625086625086625\n",
      "Epoch:  37 Training Accuracy:  0.8382809038546744\n",
      "Epoch:  37 Val Accuracy:  0.6437976437976438\n",
      "Epoch:  38 Training Accuracy:  0.8476960567124502\n",
      "Epoch:  38 Val Accuracy:  0.6625086625086625\n",
      "Epoch:  39 Training Accuracy:  0.8457022596366859\n",
      "Epoch:  39 Val Accuracy:  0.6722106722106722\n",
      "Epoch:  40 Training Accuracy:  0.882365972529907\n",
      "Epoch:  40 Val Accuracy:  0.668052668052668\n",
      "Epoch:  41 Training Accuracy:  0.882365972529907\n",
      "Epoch:  41 Val Accuracy:  0.6625086625086625\n",
      "Epoch:  42 Training Accuracy:  0.8849136021267169\n",
      "Epoch:  42 Val Accuracy:  0.6583506583506583\n",
      "Epoch:  43 Training Accuracy:  0.888458130261409\n",
      "Epoch:  43 Val Accuracy:  0.6458766458766458\n",
      "Epoch:  44 Training Accuracy:  0.8995347806823216\n",
      "Epoch:  44 Val Accuracy:  0.6534996534996536\n",
      "Epoch:  45 Training Accuracy:  0.8988701816570669\n",
      "Epoch:  45 Val Accuracy:  0.656964656964657\n",
      "Epoch:  46 Training Accuracy:  0.8965440850686752\n",
      "Epoch:  46 Val Accuracy:  0.659043659043659\n",
      "Epoch:  47 Training Accuracy:  0.905073105892778\n",
      "Epoch:  47 Val Accuracy:  0.656964656964657\n",
      "Epoch:  48 Training Accuracy:  0.9051838723969872\n",
      "Epoch:  48 Val Accuracy:  0.6534996534996536\n",
      "Epoch:  49 Training Accuracy:  0.9131590607000443\n",
      "Epoch:  49 Val Accuracy:  0.647955647955648\n",
      "Epoch:  50 Training Accuracy:  0.9068453699601241\n",
      "Epoch:  50 Val Accuracy:  0.6534996534996536\n",
      "Epoch:  51 Training Accuracy:  0.9103898980948161\n",
      "Epoch:  51 Val Accuracy:  0.6368676368676369\n",
      "Epoch:  52 Training Accuracy:  0.9123836951705804\n",
      "Epoch:  52 Val Accuracy:  0.6666666666666666\n",
      "Epoch:  53 Training Accuracy:  0.9133805937084626\n",
      "Epoch:  53 Val Accuracy:  0.6604296604296604\n",
      "Epoch:  54 Training Accuracy:  0.9160389898094816\n",
      "Epoch:  54 Val Accuracy:  0.6645876645876646\n",
      "Epoch:  55 Training Accuracy:  0.9219096145325654\n",
      "Epoch:  55 Val Accuracy:  0.6507276507276507\n",
      "Epoch:  56 Training Accuracy:  0.9241249446167479\n",
      "Epoch:  56 Val Accuracy:  0.656964656964657\n",
      "Epoch:  57 Training Accuracy:  0.9282233052724856\n",
      "Epoch:  57 Val Accuracy:  0.6410256410256411\n",
      "Epoch:  58 Training Accuracy:  0.920691182986265\n",
      "Epoch:  58 Val Accuracy:  0.6424116424116424\n",
      "Epoch:  59 Training Accuracy:  0.9253433761630483\n",
      "Epoch:  59 Val Accuracy:  0.6465696465696466\n",
      "Epoch:  60 Training Accuracy:  0.9304386353566682\n",
      "Epoch:  60 Val Accuracy:  0.6514206514206514\n",
      "Epoch:  61 Training Accuracy:  0.9339831634913602\n",
      "Epoch:  61 Val Accuracy:  0.6514206514206514\n",
      "Epoch:  62 Training Accuracy:  0.9337616304829419\n",
      "Epoch:  62 Val Accuracy:  0.6562716562716563\n",
      "Epoch:  63 Training Accuracy:  0.9337616304829419\n",
      "Epoch:  63 Val Accuracy:  0.6493416493416494\n",
      "Epoch:  64 Training Accuracy:  0.9315463003987594\n",
      "Epoch:  64 Val Accuracy:  0.6361746361746362\n",
      "Epoch:  0 Training Accuracy:  0.3455914931324767\n",
      "Epoch:  0 Val Accuracy:  0.2834372834372834\n",
      "Epoch:  1 Training Accuracy:  0.3785999113867966\n",
      "Epoch:  1 Val Accuracy:  0.28205128205128205\n",
      "Epoch:  2 Training Accuracy:  0.3254319893664156\n",
      "Epoch:  2 Val Accuracy:  0.3305613305613306\n",
      "Epoch:  3 Training Accuracy:  0.37660611431103236\n",
      "Epoch:  3 Val Accuracy:  0.23146223146223147\n",
      "Epoch:  4 Training Accuracy:  0.39377492246344703\n",
      "Epoch:  4 Val Accuracy:  0.24532224532224534\n",
      "Epoch:  5 Training Accuracy:  0.4175897208684094\n",
      "Epoch:  5 Val Accuracy:  0.28482328482328484\n",
      "Epoch:  6 Training Accuracy:  0.44993354009747455\n",
      "Epoch:  6 Val Accuracy:  0.32986832986832987\n",
      "Epoch:  7 Training Accuracy:  0.4864864864864865\n",
      "Epoch:  7 Val Accuracy:  0.31115731115731116\n",
      "Epoch:  8 Training Accuracy:  0.5036552946389012\n",
      "Epoch:  8 Val Accuracy:  0.36936936936936937\n",
      "Epoch:  9 Training Accuracy:  0.5321222862206468\n",
      "Epoch:  9 Val Accuracy:  0.35343035343035345\n",
      "Epoch:  10 Training Accuracy:  0.5446389011962782\n",
      "Epoch:  10 Val Accuracy:  0.41372141372141374\n",
      "Epoch:  11 Training Accuracy:  0.570558263181214\n",
      "Epoch:  11 Val Accuracy:  0.43173943173943174\n",
      "Epoch:  12 Training Accuracy:  0.593265396544085\n",
      "Epoch:  12 Val Accuracy:  0.4851004851004851\n",
      "Epoch:  13 Training Accuracy:  0.6436641559592379\n",
      "Epoch:  13 Val Accuracy:  0.6167706167706167\n",
      "Epoch:  14 Training Accuracy:  0.6478732831191848\n",
      "Epoch:  14 Val Accuracy:  0.6431046431046431\n",
      "Epoch:  15 Training Accuracy:  0.6538546743464776\n",
      "Epoch:  15 Val Accuracy:  0.5765765765765766\n",
      "Epoch:  16 Training Accuracy:  0.6778910057598582\n",
      "Epoch:  16 Val Accuracy:  0.571032571032571\n",
      "Epoch:  17 Training Accuracy:  0.6983828090385468\n",
      "Epoch:  17 Val Accuracy:  0.6722106722106722\n",
      "Epoch:  18 Training Accuracy:  0.6964997784669916\n",
      "Epoch:  18 Val Accuracy:  0.6424116424116424\n",
      "Epoch:  19 Training Accuracy:  0.7087948604342047\n",
      "Epoch:  19 Val Accuracy:  0.6361746361746362\n",
      "Epoch:  20 Training Accuracy:  0.7616304829419583\n",
      "Epoch:  20 Val Accuracy:  0.6534996534996536\n",
      "Epoch:  21 Training Accuracy:  0.7782454585733274\n",
      "Epoch:  21 Val Accuracy:  0.6902286902286903\n",
      "Epoch:  22 Training Accuracy:  0.781346920691183\n",
      "Epoch:  22 Val Accuracy:  0.6334026334026334\n",
      "Epoch:  23 Training Accuracy:  0.8091493132476739\n",
      "Epoch:  23 Val Accuracy:  0.6486486486486487\n",
      "Epoch:  24 Training Accuracy:  0.8235489587948605\n",
      "Epoch:  24 Val Accuracy:  0.632016632016632\n",
      "Epoch:  25 Training Accuracy:  0.8372840053167923\n",
      "Epoch:  25 Val Accuracy:  0.6202356202356203\n",
      "Epoch:  26 Training Accuracy:  0.8383916703588835\n",
      "Epoch:  26 Val Accuracy:  0.6493416493416494\n",
      "Epoch:  27 Training Accuracy:  0.8570004430660169\n",
      "Epoch:  27 Val Accuracy:  0.6583506583506583\n",
      "Epoch:  28 Training Accuracy:  0.8689632255206026\n",
      "Epoch:  28 Val Accuracy:  0.6375606375606375\n",
      "Epoch:  29 Training Accuracy:  0.8509082853345148\n",
      "Epoch:  29 Val Accuracy:  0.681912681912682\n",
      "Epoch:  30 Training Accuracy:  0.8476960567124502\n",
      "Epoch:  30 Val Accuracy:  0.6340956340956341\n",
      "Epoch:  31 Training Accuracy:  0.8845813026140895\n",
      "Epoch:  31 Val Accuracy:  0.6334026334026334\n",
      "Epoch:  32 Training Accuracy:  0.8863535666814355\n",
      "Epoch:  32 Val Accuracy:  0.6701316701316701\n",
      "Epoch:  33 Training Accuracy:  0.8946610544971201\n",
      "Epoch:  33 Val Accuracy:  0.6410256410256411\n",
      "Epoch:  34 Training Accuracy:  0.8907842268498006\n",
      "Epoch:  34 Val Accuracy:  0.6632016632016632\n",
      "Epoch:  35 Training Accuracy:  0.885024368630926\n",
      "Epoch:  35 Val Accuracy:  0.6403326403326404\n",
      "Epoch:  36 Training Accuracy:  0.912051395657953\n",
      "Epoch:  36 Val Accuracy:  0.6548856548856549\n",
      "Epoch:  37 Training Accuracy:  0.9122729286663713\n",
      "Epoch:  37 Val Accuracy:  0.6562716562716563\n",
      "Epoch:  38 Training Accuracy:  0.9157066902968543\n",
      "Epoch:  38 Val Accuracy:  0.6687456687456688\n",
      "Epoch:  39 Training Accuracy:  0.9236818785999114\n",
      "Epoch:  39 Val Accuracy:  0.65003465003465\n",
      "Epoch:  40 Training Accuracy:  0.9496012405848472\n",
      "Epoch:  40 Val Accuracy:  0.6417186417186417\n",
      "Epoch:  41 Training Accuracy:  0.9532565352237483\n",
      "Epoch:  41 Val Accuracy:  0.6625086625086625\n",
      "Epoch:  42 Training Accuracy:  0.9574656623836951\n",
      "Epoch:  42 Val Accuracy:  0.6791406791406791\n",
      "Epoch:  43 Training Accuracy:  0.9571333628710678\n",
      "Epoch:  43 Val Accuracy:  0.6645876645876646\n",
      "Epoch:  44 Training Accuracy:  0.9551395657953035\n",
      "Epoch:  44 Val Accuracy:  0.6521136521136521\n",
      "Epoch:  45 Training Accuracy:  0.9628932210899424\n",
      "Epoch:  45 Val Accuracy:  0.6507276507276507\n",
      "Epoch:  46 Training Accuracy:  0.9669915817456801\n",
      "Epoch:  46 Val Accuracy:  0.656964656964657\n",
      "Epoch:  47 Training Accuracy:  0.9684315463003987\n",
      "Epoch:  47 Val Accuracy:  0.6618156618156619\n",
      "Epoch:  48 Training Accuracy:  0.969428444838281\n",
      "Epoch:  48 Val Accuracy:  0.6472626472626473\n",
      "Epoch:  49 Training Accuracy:  0.9640008861320337\n",
      "Epoch:  49 Val Accuracy:  0.6541926541926542\n",
      "Epoch:  50 Training Accuracy:  0.9708684093929996\n",
      "Epoch:  50 Val Accuracy:  0.6431046431046431\n",
      "Epoch:  51 Training Accuracy:  0.9712007089056269\n",
      "Epoch:  51 Val Accuracy:  0.6528066528066528\n",
      "Epoch:  52 Training Accuracy:  0.9759636685866194\n",
      "Epoch:  52 Val Accuracy:  0.6576576576576577\n",
      "Epoch:  53 Training Accuracy:  0.9779574656623837\n",
      "Epoch:  53 Val Accuracy:  0.65003465003465\n",
      "Epoch:  54 Training Accuracy:  0.981723526805494\n",
      "Epoch:  54 Val Accuracy:  0.6437976437976438\n",
      "Epoch:  55 Training Accuracy:  0.9749667700487372\n",
      "Epoch:  55 Val Accuracy:  0.647955647955648\n",
      "Epoch:  56 Training Accuracy:  0.9679884802835622\n",
      "Epoch:  56 Val Accuracy:  0.6625086625086625\n",
      "Epoch:  57 Training Accuracy:  0.9687638458130261\n",
      "Epoch:  57 Val Accuracy:  0.6652806652806653\n",
      "Epoch:  58 Training Accuracy:  0.9802835622507754\n",
      "Epoch:  58 Val Accuracy:  0.6437976437976438\n",
      "Epoch:  59 Training Accuracy:  0.9846034559149314\n",
      "Epoch:  59 Val Accuracy:  0.6403326403326404\n",
      "Epoch:  60 Training Accuracy:  0.9883695170580417\n",
      "Epoch:  60 Val Accuracy:  0.6534996534996536\n",
      "Epoch:  61 Training Accuracy:  0.9893664155959238\n",
      "Epoch:  61 Val Accuracy:  0.6451836451836452\n",
      "Epoch:  62 Training Accuracy:  0.9883695170580417\n",
      "Epoch:  62 Val Accuracy:  0.6528066528066528\n",
      "Epoch:  63 Training Accuracy:  0.9893664155959238\n",
      "Epoch:  63 Val Accuracy:  0.6541926541926542\n",
      "Epoch:  64 Training Accuracy:  0.9892556490917147\n",
      "Epoch:  64 Val Accuracy:  0.65003465003465\n",
      "Epoch:  65 Training Accuracy:  0.9902525476295968\n",
      "Epoch:  65 Val Accuracy:  0.6541926541926542\n",
      "Epoch:  66 Training Accuracy:  0.9890341160832964\n",
      "Epoch:  66 Val Accuracy:  0.6514206514206514\n",
      "Epoch:  67 Training Accuracy:  0.9896987151085511\n",
      "Epoch:  67 Val Accuracy:  0.6493416493416494\n",
      "Epoch:  0 Training Accuracy:  0.3070447496677005\n",
      "Epoch:  0 Val Accuracy:  0.19265419265419265\n",
      "Epoch:  1 Training Accuracy:  0.28156845369960126\n",
      "Epoch:  1 Val Accuracy:  0.2875952875952876\n",
      "Epoch:  2 Training Accuracy:  0.306601683650864\n",
      "Epoch:  2 Val Accuracy:  0.1864171864171864\n",
      "Epoch:  3 Training Accuracy:  0.2896544085068675\n",
      "Epoch:  3 Val Accuracy:  0.17671517671517672\n",
      "Epoch:  4 Training Accuracy:  0.28599911386796634\n",
      "Epoch:  4 Val Accuracy:  0.18156618156618157\n",
      "Epoch:  5 Training Accuracy:  0.2967434647762517\n",
      "Epoch:  5 Val Accuracy:  0.2591822591822592\n",
      "Epoch:  6 Training Accuracy:  0.30815241470979177\n",
      "Epoch:  6 Val Accuracy:  0.35065835065835066\n",
      "Epoch:  7 Training Accuracy:  0.25875055383252105\n",
      "Epoch:  7 Val Accuracy:  0.18988218988218988\n",
      "Epoch:  8 Training Accuracy:  0.3156845369960124\n",
      "Epoch:  8 Val Accuracy:  0.35065835065835066\n",
      "Epoch:  9 Training Accuracy:  0.34935755427558707\n",
      "Epoch:  9 Val Accuracy:  0.367983367983368\n",
      "Epoch:  10 Training Accuracy:  0.3287549844926894\n",
      "Epoch:  10 Val Accuracy:  0.3444213444213444\n",
      "Epoch:  11 Training Accuracy:  0.3520159503766061\n",
      "Epoch:  11 Val Accuracy:  0.4067914067914068\n",
      "Epoch:  12 Training Accuracy:  0.3470314576871954\n",
      "Epoch:  12 Val Accuracy:  0.45807345807345806\n",
      "Epoch:  13 Training Accuracy:  0.3686309260079752\n",
      "Epoch:  13 Val Accuracy:  0.3749133749133749\n",
      "Epoch:  14 Training Accuracy:  0.36896322552060257\n",
      "Epoch:  14 Val Accuracy:  0.3880803880803881\n",
      "Epoch:  15 Training Accuracy:  0.39975631369073994\n",
      "Epoch:  15 Val Accuracy:  0.3832293832293832\n",
      "Epoch:  16 Training Accuracy:  0.3751661497563137\n",
      "Epoch:  16 Val Accuracy:  0.2772002772002772\n",
      "Epoch:  17 Training Accuracy:  0.405073105892778\n",
      "Epoch:  17 Val Accuracy:  0.352044352044352\n",
      "Epoch:  18 Training Accuracy:  0.3270934869295525\n",
      "Epoch:  18 Val Accuracy:  0.23007623007623007\n",
      "Epoch:  19 Training Accuracy:  0.39222419140451925\n",
      "Epoch:  19 Val Accuracy:  0.4358974358974359\n",
      "Epoch:  20 Training Accuracy:  0.4160389898094816\n",
      "Epoch:  20 Val Accuracy:  0.3659043659043659\n",
      "Epoch:  21 Training Accuracy:  0.44162605228178997\n",
      "Epoch:  21 Val Accuracy:  0.3943173943173943\n",
      "Epoch:  22 Training Accuracy:  0.45425343376163047\n",
      "Epoch:  22 Val Accuracy:  0.37075537075537074\n",
      "Epoch:  23 Training Accuracy:  0.47518830305715554\n",
      "Epoch:  23 Val Accuracy:  0.3762993762993763\n",
      "Epoch:  24 Training Accuracy:  0.4720868409393\n",
      "Epoch:  24 Val Accuracy:  0.39986139986139985\n",
      "Epoch:  25 Training Accuracy:  0.48792645104120513\n",
      "Epoch:  25 Val Accuracy:  0.38253638253638256\n",
      "Epoch:  26 Training Accuracy:  0.5027691626052282\n",
      "Epoch:  26 Val Accuracy:  0.43936243936243935\n",
      "Epoch:  27 Training Accuracy:  0.5111874169251218\n",
      "Epoch:  27 Val Accuracy:  0.47124047124047125\n",
      "Epoch:  28 Training Accuracy:  0.5224856003544528\n",
      "Epoch:  28 Val Accuracy:  0.46916146916146917\n",
      "Epoch:  29 Training Accuracy:  0.5394328754984493\n",
      "Epoch:  29 Val Accuracy:  0.4698544698544699\n",
      "Epoch:  30 Training Accuracy:  0.5309038546743465\n",
      "Epoch:  30 Val Accuracy:  0.48440748440748443\n",
      "Epoch:  31 Training Accuracy:  0.5292423571112096\n",
      "Epoch:  31 Val Accuracy:  0.4954954954954955\n",
      "Epoch:  32 Training Accuracy:  0.5399867080194949\n",
      "Epoch:  32 Val Accuracy:  0.5446985446985447\n",
      "Epoch:  33 Training Accuracy:  0.5587062472308374\n",
      "Epoch:  33 Val Accuracy:  0.4941094941094941\n",
      "Epoch:  34 Training Accuracy:  0.5388790429774036\n",
      "Epoch:  34 Val Accuracy:  0.5363825363825364\n",
      "Epoch:  35 Training Accuracy:  0.5870624723083739\n",
      "Epoch:  35 Val Accuracy:  0.5433125433125433\n",
      "Epoch:  36 Training Accuracy:  0.5521710234824989\n",
      "Epoch:  36 Val Accuracy:  0.5211365211365211\n",
      "Epoch:  37 Training Accuracy:  0.5721089942401418\n",
      "Epoch:  37 Val Accuracy:  0.5412335412335413\n",
      "Epoch:  38 Training Accuracy:  0.5818564466105449\n",
      "Epoch:  38 Val Accuracy:  0.5550935550935551\n",
      "Epoch:  39 Training Accuracy:  0.5957022596366859\n",
      "Epoch:  39 Val Accuracy:  0.5641025641025641\n",
      "Epoch:  40 Training Accuracy:  0.6104342046964998\n",
      "Epoch:  40 Val Accuracy:  0.5661815661815662\n",
      "Epoch:  41 Training Accuracy:  0.6081081081081081\n",
      "Epoch:  41 Val Accuracy:  0.5668745668745668\n",
      "Epoch:  42 Training Accuracy:  0.6107665042091271\n",
      "Epoch:  42 Val Accuracy:  0.589050589050589\n",
      "Epoch:  43 Training Accuracy:  0.6054497120070891\n",
      "Epoch:  43 Val Accuracy:  0.5904365904365905\n",
      "Epoch:  44 Training Accuracy:  0.6050066459902526\n",
      "Epoch:  44 Val Accuracy:  0.5918225918225918\n",
      "Epoch:  45 Training Accuracy:  0.6143110323438192\n",
      "Epoch:  45 Val Accuracy:  0.5862785862785863\n",
      "Epoch:  46 Training Accuracy:  0.6209570225963669\n",
      "Epoch:  46 Val Accuracy:  0.5918225918225918\n",
      "Epoch:  47 Training Accuracy:  0.6216216216216216\n",
      "Epoch:  47 Val Accuracy:  0.5758835758835759\n",
      "Epoch:  48 Training Accuracy:  0.615861763402747\n",
      "Epoch:  48 Val Accuracy:  0.5959805959805959\n",
      "Epoch:  49 Training Accuracy:  0.6252769162605228\n",
      "Epoch:  49 Val Accuracy:  0.5675675675675675\n",
      "Epoch:  50 Training Accuracy:  0.6231723526805494\n",
      "Epoch:  50 Val Accuracy:  0.5959805959805959\n",
      "Epoch:  51 Training Accuracy:  0.6328090385467434\n",
      "Epoch:  51 Val Accuracy:  0.5731115731115731\n",
      "Epoch:  52 Training Accuracy:  0.6284891448825876\n",
      "Epoch:  52 Val Accuracy:  0.5703395703395704\n",
      "Epoch:  53 Training Accuracy:  0.6249446167478955\n",
      "Epoch:  53 Val Accuracy:  0.5904365904365905\n",
      "Epoch:  54 Training Accuracy:  0.6233938856889677\n",
      "Epoch:  54 Val Accuracy:  0.6077616077616078\n",
      "Epoch:  55 Training Accuracy:  0.6251661497563137\n",
      "Epoch:  55 Val Accuracy:  0.604989604989605\n",
      "Epoch:  0 Training Accuracy:  0.31745680106335844\n",
      "Epoch:  0 Val Accuracy:  0.2203742203742204\n",
      "Epoch:  1 Training Accuracy:  0.25753212228622063\n",
      "Epoch:  1 Val Accuracy:  0.13236313236313235\n",
      "Epoch:  2 Training Accuracy:  0.34049623393885686\n",
      "Epoch:  2 Val Accuracy:  0.18503118503118504\n",
      "Epoch:  3 Training Accuracy:  0.3283119184758529\n",
      "Epoch:  3 Val Accuracy:  0.2598752598752599\n",
      "Epoch:  4 Training Accuracy:  0.3309703145768719\n",
      "Epoch:  4 Val Accuracy:  0.2203742203742204\n",
      "Epoch:  5 Training Accuracy:  0.36852015950376604\n",
      "Epoch:  5 Val Accuracy:  0.34026334026334026\n",
      "Epoch:  6 Training Accuracy:  0.38159060700044306\n",
      "Epoch:  6 Val Accuracy:  0.4095634095634096\n",
      "Epoch:  7 Training Accuracy:  0.3751661497563137\n",
      "Epoch:  7 Val Accuracy:  0.3665973665973666\n",
      "Epoch:  8 Training Accuracy:  0.4583517944173682\n",
      "Epoch:  8 Val Accuracy:  0.4435204435204435\n",
      "Epoch:  9 Training Accuracy:  0.5175011076650421\n",
      "Epoch:  9 Val Accuracy:  0.4442134442134442\n",
      "Epoch:  10 Training Accuracy:  0.5446389011962782\n",
      "Epoch:  10 Val Accuracy:  0.4954954954954955\n",
      "Epoch:  11 Training Accuracy:  0.5494018608772707\n",
      "Epoch:  11 Val Accuracy:  0.49965349965349964\n",
      "Epoch:  12 Training Accuracy:  0.5890562693841382\n",
      "Epoch:  12 Val Accuracy:  0.6098406098406098\n",
      "Epoch:  13 Training Accuracy:  0.6252769162605228\n",
      "Epoch:  13 Val Accuracy:  0.6146916146916147\n",
      "Epoch:  14 Training Accuracy:  0.6418918918918919\n",
      "Epoch:  14 Val Accuracy:  0.6271656271656272\n",
      "Epoch:  15 Training Accuracy:  0.6074435090828534\n",
      "Epoch:  15 Val Accuracy:  0.6410256410256411\n",
      "Epoch:  16 Training Accuracy:  0.621067789100576\n",
      "Epoch:  16 Val Accuracy:  0.6216216216216216\n",
      "Epoch:  17 Training Accuracy:  0.6274922463447054\n",
      "Epoch:  17 Val Accuracy:  0.6105336105336105\n",
      "Epoch:  18 Training Accuracy:  0.6659282233052725\n",
      "Epoch:  18 Val Accuracy:  0.6444906444906445\n",
      "Epoch:  19 Training Accuracy:  0.6761187416925122\n",
      "Epoch:  19 Val Accuracy:  0.7408177408177408\n",
      "Epoch:  20 Training Accuracy:  0.7113424900310146\n",
      "Epoch:  20 Val Accuracy:  0.7089397089397089\n",
      "Epoch:  21 Training Accuracy:  0.717766947275144\n",
      "Epoch:  21 Val Accuracy:  0.7047817047817048\n",
      "Epoch:  22 Training Accuracy:  0.7216437749224635\n",
      "Epoch:  22 Val Accuracy:  0.7151767151767152\n",
      "Epoch:  23 Training Accuracy:  0.722972972972973\n",
      "Epoch:  23 Val Accuracy:  0.7255717255717256\n",
      "Epoch:  24 Training Accuracy:  0.7408063801506425\n",
      "Epoch:  24 Val Accuracy:  0.6846846846846847\n",
      "Epoch:  25 Training Accuracy:  0.7361541869738591\n",
      "Epoch:  25 Val Accuracy:  0.717948717948718\n",
      "Epoch:  26 Training Accuracy:  0.7536552946389012\n",
      "Epoch:  26 Val Accuracy:  0.6923076923076923\n",
      "Epoch:  27 Training Accuracy:  0.7609658839167036\n",
      "Epoch:  27 Val Accuracy:  0.7013167013167013\n",
      "Epoch:  28 Training Accuracy:  0.7618520159503767\n",
      "Epoch:  28 Val Accuracy:  0.6916146916146916\n",
      "Epoch:  29 Training Accuracy:  0.7740363314133806\n",
      "Epoch:  29 Val Accuracy:  0.6964656964656964\n",
      "Epoch:  30 Training Accuracy:  0.7782454585733274\n",
      "Epoch:  30 Val Accuracy:  0.6978516978516979\n",
      "Epoch:  31 Training Accuracy:  0.7758085954807267\n",
      "Epoch:  31 Val Accuracy:  0.6909216909216909\n",
      "Epoch:  32 Training Accuracy:  0.7518830305715551\n",
      "Epoch:  32 Val Accuracy:  0.7165627165627165\n",
      "Epoch:  33 Training Accuracy:  0.76650420912716\n",
      "Epoch:  33 Val Accuracy:  0.6909216909216909\n",
      "Epoch:  34 Training Accuracy:  0.7671688081524147\n",
      "Epoch:  34 Val Accuracy:  0.7089397089397089\n",
      "Epoch:  35 Training Accuracy:  0.7971865307930882\n",
      "Epoch:  35 Val Accuracy:  0.7089397089397089\n",
      "Epoch:  36 Training Accuracy:  0.7956357997341604\n",
      "Epoch:  36 Val Accuracy:  0.695079695079695\n",
      "Epoch:  37 Training Accuracy:  0.784780682321666\n",
      "Epoch:  37 Val Accuracy:  0.7366597366597366\n",
      "Epoch:  38 Training Accuracy:  0.7742578644217989\n",
      "Epoch:  38 Val Accuracy:  0.695079695079695\n",
      "Epoch:  39 Training Accuracy:  0.790097474523704\n",
      "Epoch:  39 Val Accuracy:  0.683991683991684\n",
      "Epoch:  40 Training Accuracy:  0.8162383695170581\n",
      "Epoch:  40 Val Accuracy:  0.7214137214137214\n",
      "Epoch:  41 Training Accuracy:  0.817013735046522\n",
      "Epoch:  41 Val Accuracy:  0.7255717255717256\n",
      "Epoch:  42 Training Accuracy:  0.8259858218874613\n",
      "Epoch:  42 Val Accuracy:  0.717948717948718\n",
      "Epoch:  43 Training Accuracy:  0.830970314576872\n",
      "Epoch:  43 Val Accuracy:  0.7304227304227304\n",
      "Epoch:  44 Training Accuracy:  0.8392778023925564\n",
      "Epoch:  44 Val Accuracy:  0.704088704088704\n",
      "Epoch:  45 Training Accuracy:  0.8377270713336287\n",
      "Epoch:  45 Val Accuracy:  0.7221067221067221\n",
      "Epoch:  46 Training Accuracy:  0.8403854674346478\n",
      "Epoch:  46 Val Accuracy:  0.7054747054747055\n",
      "Epoch:  47 Training Accuracy:  0.8472529906956137\n",
      "Epoch:  47 Val Accuracy:  0.7338877338877339\n",
      "Epoch:  48 Training Accuracy:  0.8416038989809481\n",
      "Epoch:  48 Val Accuracy:  0.726957726957727\n",
      "Epoch:  49 Training Accuracy:  0.8483606557377049\n",
      "Epoch:  49 Val Accuracy:  0.726957726957727\n",
      "Epoch:  50 Training Accuracy:  0.8565573770491803\n",
      "Epoch:  50 Val Accuracy:  0.7304227304227304\n",
      "Epoch:  51 Training Accuracy:  0.8651971643774923\n",
      "Epoch:  51 Val Accuracy:  0.7338877338877339\n",
      "Epoch:  52 Training Accuracy:  0.8705139565795303\n",
      "Epoch:  52 Val Accuracy:  0.7221067221067221\n",
      "Epoch:  53 Training Accuracy:  0.8613203367301728\n",
      "Epoch:  53 Val Accuracy:  0.7255717255717256\n",
      "Epoch:  54 Training Accuracy:  0.8708462560921577\n",
      "Epoch:  54 Val Accuracy:  0.7214137214137214\n",
      "Epoch:  55 Training Accuracy:  0.8758307487815684\n",
      "Epoch:  55 Val Accuracy:  0.7241857241857241\n",
      "Epoch:  56 Training Accuracy:  0.871067789100576\n",
      "Epoch:  56 Val Accuracy:  0.7207207207207207\n",
      "Epoch:  57 Training Accuracy:  0.8749446167478955\n",
      "Epoch:  57 Val Accuracy:  0.7130977130977131\n",
      "Epoch:  58 Training Accuracy:  0.8819229065130705\n",
      "Epoch:  58 Val Accuracy:  0.7006237006237006\n",
      "Epoch:  59 Training Accuracy:  0.892002658396101\n",
      "Epoch:  59 Val Accuracy:  0.7130977130977131\n",
      "Epoch:  60 Training Accuracy:  0.8966548515728844\n",
      "Epoch:  60 Val Accuracy:  0.7144837144837145\n",
      "Epoch:  61 Training Accuracy:  0.8953256535223748\n",
      "Epoch:  61 Val Accuracy:  0.717948717948718\n",
      "Epoch:  62 Training Accuracy:  0.8975409836065574\n",
      "Epoch:  62 Val Accuracy:  0.7304227304227304\n",
      "Epoch:  63 Training Accuracy:  0.8975409836065574\n",
      "Epoch:  63 Val Accuracy:  0.7124047124047124\n",
      "Epoch:  64 Training Accuracy:  0.9005316792202038\n",
      "Epoch:  64 Val Accuracy:  0.7137907137907138\n",
      "Epoch:  65 Training Accuracy:  0.9008639787328312\n",
      "Epoch:  65 Val Accuracy:  0.7137907137907138\n",
      "Epoch:  66 Training Accuracy:  0.9061807709348693\n",
      "Epoch:  66 Val Accuracy:  0.7193347193347194\n",
      "Epoch:  67 Training Accuracy:  0.9073992024811697\n",
      "Epoch:  67 Val Accuracy:  0.7137907137907138\n",
      "Epoch:  68 Training Accuracy:  0.9045192733717324\n",
      "Epoch:  68 Val Accuracy:  0.7227997227997228\n",
      "Epoch:  69 Training Accuracy:  0.905959237926451\n",
      "Epoch:  69 Val Accuracy:  0.7013167013167013\n",
      "Epoch:  0 Training Accuracy:  0.3463668586619406\n",
      "Epoch:  0 Val Accuracy:  0.21136521136521136\n",
      "Epoch:  1 Training Accuracy:  0.26240584847142223\n",
      "Epoch:  1 Val Accuracy:  0.1275121275121275\n",
      "Epoch:  2 Training Accuracy:  0.28943287549844926\n",
      "Epoch:  2 Val Accuracy:  0.21483021483021483\n",
      "Epoch:  3 Training Accuracy:  0.2625166149756314\n",
      "Epoch:  3 Val Accuracy:  0.4594594594594595\n",
      "Epoch:  4 Training Accuracy:  0.3304164820558263\n",
      "Epoch:  4 Val Accuracy:  0.27373527373527373\n",
      "Epoch:  5 Training Accuracy:  0.26151971643774924\n",
      "Epoch:  5 Val Accuracy:  0.11988911988911989\n",
      "Epoch:  6 Training Accuracy:  0.30482941958351795\n",
      "Epoch:  6 Val Accuracy:  0.2577962577962578\n",
      "Epoch:  7 Training Accuracy:  0.3714000886132034\n",
      "Epoch:  7 Val Accuracy:  0.42411642411642414\n",
      "Epoch:  8 Training Accuracy:  0.35855117412494464\n",
      "Epoch:  8 Val Accuracy:  0.28135828135828134\n",
      "Epoch:  9 Training Accuracy:  0.43420469649977844\n",
      "Epoch:  9 Val Accuracy:  0.44005544005544006\n",
      "Epoch:  10 Training Accuracy:  0.4713114754098361\n",
      "Epoch:  10 Val Accuracy:  0.5107415107415108\n",
      "Epoch:  11 Training Accuracy:  0.4805050952591936\n",
      "Epoch:  11 Val Accuracy:  0.5848925848925849\n",
      "Epoch:  12 Training Accuracy:  0.5674568010633585\n",
      "Epoch:  12 Val Accuracy:  0.4268884268884269\n",
      "Epoch:  13 Training Accuracy:  0.5867301727957466\n",
      "Epoch:  13 Val Accuracy:  0.5661815661815662\n",
      "Epoch:  14 Training Accuracy:  0.6281568453699601\n",
      "Epoch:  14 Val Accuracy:  0.6181566181566182\n",
      "Epoch:  15 Training Accuracy:  0.6290429774036331\n",
      "Epoch:  15 Val Accuracy:  0.6313236313236313\n",
      "Epoch:  16 Training Accuracy:  0.6616083296411165\n",
      "Epoch:  16 Val Accuracy:  0.6465696465696466\n",
      "Epoch:  17 Training Accuracy:  0.6591714665485158\n",
      "Epoch:  17 Val Accuracy:  0.623007623007623\n",
      "Epoch:  18 Training Accuracy:  0.6751218431546301\n",
      "Epoch:  18 Val Accuracy:  0.7221067221067221\n",
      "Epoch:  19 Training Accuracy:  0.6476517501107665\n",
      "Epoch:  19 Val Accuracy:  0.6354816354816355\n",
      "Epoch:  20 Training Accuracy:  0.6791094373061586\n",
      "Epoch:  20 Val Accuracy:  0.6458766458766458\n",
      "Epoch:  21 Training Accuracy:  0.6640451927337173\n",
      "Epoch:  21 Val Accuracy:  0.6645876645876646\n",
      "Epoch:  22 Training Accuracy:  0.7053610988037218\n",
      "Epoch:  22 Val Accuracy:  0.6902286902286903\n",
      "Epoch:  23 Training Accuracy:  0.7114532565352237\n",
      "Epoch:  23 Val Accuracy:  0.6860706860706861\n",
      "Epoch:  24 Training Accuracy:  0.7124501550731059\n",
      "Epoch:  24 Val Accuracy:  0.6666666666666666\n",
      "Epoch:  25 Training Accuracy:  0.7261852015950376\n",
      "Epoch:  25 Val Accuracy:  0.6992376992376992\n",
      "Epoch:  26 Training Accuracy:  0.7280682321665928\n",
      "Epoch:  26 Val Accuracy:  0.7006237006237006\n",
      "Epoch:  27 Training Accuracy:  0.7410279131590607\n",
      "Epoch:  27 Val Accuracy:  0.6867636867636867\n",
      "Epoch:  28 Training Accuracy:  0.7465662383695171\n",
      "Epoch:  28 Val Accuracy:  0.7096327096327096\n",
      "Epoch:  29 Training Accuracy:  0.7433540097474524\n",
      "Epoch:  29 Val Accuracy:  0.6971586971586972\n",
      "Epoch:  30 Training Accuracy:  0.7540983606557377\n",
      "Epoch:  30 Val Accuracy:  0.6763686763686764\n",
      "Epoch:  31 Training Accuracy:  0.7410279131590607\n",
      "Epoch:  31 Val Accuracy:  0.6791406791406791\n",
      "Epoch:  32 Training Accuracy:  0.7544306601683651\n",
      "Epoch:  32 Val Accuracy:  0.7186417186417187\n",
      "Epoch:  33 Training Accuracy:  0.7635135135135135\n",
      "Epoch:  33 Val Accuracy:  0.6687456687456688\n",
      "Epoch:  34 Training Accuracy:  0.7657288435976961\n",
      "Epoch:  34 Val Accuracy:  0.7061677061677062\n",
      "Epoch:  35 Training Accuracy:  0.8017279574656624\n",
      "Epoch:  35 Val Accuracy:  0.683991683991684\n",
      "Epoch:  36 Training Accuracy:  0.7852237483385024\n",
      "Epoch:  36 Val Accuracy:  0.6867636867636867\n",
      "Epoch:  37 Training Accuracy:  0.791758972086841\n",
      "Epoch:  37 Val Accuracy:  0.6777546777546778\n",
      "Epoch:  38 Training Accuracy:  0.8077093486929553\n",
      "Epoch:  38 Val Accuracy:  0.6978516978516979\n",
      "Epoch:  39 Training Accuracy:  0.8125830748781568\n",
      "Epoch:  39 Val Accuracy:  0.6971586971586972\n",
      "Epoch:  40 Training Accuracy:  0.8478068232166592\n",
      "Epoch:  40 Val Accuracy:  0.702009702009702\n",
      "Epoch:  41 Training Accuracy:  0.860655737704918\n",
      "Epoch:  41 Val Accuracy:  0.6888426888426888\n",
      "Epoch:  42 Training Accuracy:  0.8667478954364201\n",
      "Epoch:  42 Val Accuracy:  0.7061677061677062\n",
      "Epoch:  43 Training Accuracy:  0.8690739920248117\n",
      "Epoch:  43 Val Accuracy:  0.7130977130977131\n",
      "Epoch:  44 Training Accuracy:  0.8784891448825876\n",
      "Epoch:  44 Val Accuracy:  0.7054747054747055\n",
      "Epoch:  45 Training Accuracy:  0.8802614089499335\n",
      "Epoch:  45 Val Accuracy:  0.6777546777546778\n",
      "Epoch:  46 Training Accuracy:  0.877159946832078\n",
      "Epoch:  46 Val Accuracy:  0.6936936936936937\n",
      "Epoch:  47 Training Accuracy:  0.8861320336730173\n",
      "Epoch:  47 Val Accuracy:  0.6909216909216909\n",
      "Epoch:  48 Training Accuracy:  0.8925564909171466\n",
      "Epoch:  48 Val Accuracy:  0.6888426888426888\n",
      "Epoch:  49 Training Accuracy:  0.8955471865307931\n",
      "Epoch:  49 Val Accuracy:  0.6985446985446986\n",
      "Epoch:  50 Training Accuracy:  0.9011962782454586\n",
      "Epoch:  50 Val Accuracy:  0.6874566874566874\n",
      "Epoch:  51 Training Accuracy:  0.9049623393885688\n",
      "Epoch:  51 Val Accuracy:  0.6853776853776854\n",
      "Epoch:  52 Training Accuracy:  0.91371289322109\n",
      "Epoch:  52 Val Accuracy:  0.6943866943866944\n",
      "Epoch:  53 Training Accuracy:  0.91371289322109\n",
      "Epoch:  53 Val Accuracy:  0.6867636867636867\n",
      "Epoch:  54 Training Accuracy:  0.8953256535223748\n",
      "Epoch:  54 Val Accuracy:  0.6999306999306999\n",
      "Epoch:  55 Training Accuracy:  0.9205804164820558\n",
      "Epoch:  55 Val Accuracy:  0.6985446985446986\n",
      "Epoch:  56 Training Accuracy:  0.9204696499778467\n",
      "Epoch:  56 Val Accuracy:  0.6909216909216909\n",
      "Epoch:  57 Training Accuracy:  0.9191404519273372\n",
      "Epoch:  57 Val Accuracy:  0.6722106722106722\n",
      "Epoch:  58 Training Accuracy:  0.9275587062472308\n",
      "Epoch:  58 Val Accuracy:  0.702009702009702\n",
      "Epoch:  59 Training Accuracy:  0.9309924678777137\n",
      "Epoch:  59 Val Accuracy:  0.6881496881496881\n",
      "Epoch:  60 Training Accuracy:  0.9391891891891891\n",
      "Epoch:  60 Val Accuracy:  0.6895356895356896\n",
      "Epoch:  61 Training Accuracy:  0.9409614532565352\n",
      "Epoch:  61 Val Accuracy:  0.6971586971586972\n",
      "Epoch:  62 Training Accuracy:  0.9446167478954364\n",
      "Epoch:  62 Val Accuracy:  0.693000693000693\n",
      "Epoch:  63 Training Accuracy:  0.9391891891891891\n",
      "Epoch:  63 Val Accuracy:  0.6985446985446986\n",
      "Epoch:  64 Training Accuracy:  0.9448382809038547\n",
      "Epoch:  64 Val Accuracy:  0.704088704088704\n",
      "Epoch:  65 Training Accuracy:  0.9443952148870182\n",
      "Epoch:  65 Val Accuracy:  0.7047817047817048\n",
      "Epoch:  66 Training Accuracy:  0.9460567124501551\n",
      "Epoch:  66 Val Accuracy:  0.7033957033957033\n",
      "Epoch:  67 Training Accuracy:  0.9478289765175011\n",
      "Epoch:  67 Val Accuracy:  0.7013167013167013\n",
      "Epoch:  68 Training Accuracy:  0.9509304386353566\n",
      "Epoch:  68 Val Accuracy:  0.6992376992376992\n",
      "Epoch:  69 Training Accuracy:  0.9536996012405848\n",
      "Epoch:  69 Val Accuracy:  0.6923076923076923\n",
      "Epoch:  70 Training Accuracy:  0.9539211342490032\n",
      "Epoch:  70 Val Accuracy:  0.6957726957726957\n",
      "Epoch:  71 Training Accuracy:  0.9519273371732389\n",
      "Epoch:  71 Val Accuracy:  0.6881496881496881\n",
      "Epoch:  0 Training Accuracy:  0.3624280017722641\n",
      "Epoch:  0 Val Accuracy:  0.18988218988218988\n",
      "Epoch:  1 Training Accuracy:  0.37339388568896764\n",
      "Epoch:  1 Val Accuracy:  0.22522522522522523\n",
      "Epoch:  2 Training Accuracy:  0.40872840053167925\n",
      "Epoch:  2 Val Accuracy:  0.2591822591822592\n",
      "Epoch:  3 Training Accuracy:  0.44350908285334517\n",
      "Epoch:  3 Val Accuracy:  0.29036729036729036\n",
      "Epoch:  4 Training Accuracy:  0.41936198493575544\n",
      "Epoch:  4 Val Accuracy:  0.33957033957033955\n",
      "Epoch:  5 Training Accuracy:  0.46112095702259637\n",
      "Epoch:  5 Val Accuracy:  0.2910602910602911\n",
      "Epoch:  6 Training Accuracy:  0.4132698272042534\n",
      "Epoch:  6 Val Accuracy:  0.23354123354123354\n",
      "Epoch:  7 Training Accuracy:  0.4539211342490031\n",
      "Epoch:  7 Val Accuracy:  0.29244629244629244\n",
      "Epoch:  8 Training Accuracy:  0.44583517944173684\n",
      "Epoch:  8 Val Accuracy:  0.4144144144144144\n",
      "Epoch:  9 Training Accuracy:  0.4935755427558706\n",
      "Epoch:  9 Val Accuracy:  0.4144144144144144\n",
      "Epoch:  10 Training Accuracy:  0.5297961896322552\n",
      "Epoch:  10 Val Accuracy:  0.4019404019404019\n",
      "Epoch:  11 Training Accuracy:  0.5137350465219318\n",
      "Epoch:  11 Val Accuracy:  0.37976437976437977\n",
      "Epoch:  12 Training Accuracy:  0.5344483828090385\n",
      "Epoch:  12 Val Accuracy:  0.4123354123354123\n",
      "Epoch:  13 Training Accuracy:  0.562693841382366\n",
      "Epoch:  13 Val Accuracy:  0.45807345807345806\n",
      "Epoch:  14 Training Accuracy:  0.5573770491803278\n",
      "Epoch:  14 Val Accuracy:  0.4885654885654886\n",
      "Epoch:  15 Training Accuracy:  0.48604342046965\n",
      "Epoch:  15 Val Accuracy:  0.3887733887733888\n",
      "Epoch:  16 Training Accuracy:  0.5405405405405406\n",
      "Epoch:  16 Val Accuracy:  0.4151074151074151\n",
      "Epoch:  17 Training Accuracy:  0.5795303500221533\n",
      "Epoch:  17 Val Accuracy:  0.46015246015246014\n",
      "Epoch:  18 Training Accuracy:  0.6066681435533895\n",
      "Epoch:  18 Val Accuracy:  0.5488565488565489\n",
      "Epoch:  19 Training Accuracy:  0.5944838280903855\n",
      "Epoch:  19 Val Accuracy:  0.5336105336105336\n",
      "Epoch:  20 Training Accuracy:  0.6226185201595038\n",
      "Epoch:  20 Val Accuracy:  0.5370755370755371\n",
      "Epoch:  21 Training Accuracy:  0.6441072219760744\n",
      "Epoch:  21 Val Accuracy:  0.5641025641025641\n",
      "Epoch:  22 Training Accuracy:  0.6389011962782455\n",
      "Epoch:  22 Val Accuracy:  0.5557865557865558\n",
      "Epoch:  23 Training Accuracy:  0.6438856889676562\n",
      "Epoch:  23 Val Accuracy:  0.5557865557865558\n",
      "Epoch:  24 Training Accuracy:  0.650753212228622\n",
      "Epoch:  24 Val Accuracy:  0.5883575883575883\n",
      "Epoch:  25 Training Accuracy:  0.6387904297740363\n",
      "Epoch:  25 Val Accuracy:  0.5585585585585585\n",
      "Epoch:  26 Training Accuracy:  0.6520824102791316\n",
      "Epoch:  26 Val Accuracy:  0.5675675675675675\n",
      "Epoch:  27 Training Accuracy:  0.655073105892778\n",
      "Epoch:  27 Val Accuracy:  0.577962577962578\n",
      "Epoch:  28 Training Accuracy:  0.6667035888347363\n",
      "Epoch:  28 Val Accuracy:  0.5592515592515592\n",
      "Epoch:  29 Training Accuracy:  0.6595037660611431\n",
      "Epoch:  29 Val Accuracy:  0.5717255717255717\n",
      "Epoch:  30 Training Accuracy:  0.6285999113867966\n",
      "Epoch:  30 Val Accuracy:  0.5758835758835759\n",
      "Epoch:  31 Training Accuracy:  0.6515285777580859\n",
      "Epoch:  31 Val Accuracy:  0.6015246015246015\n",
      "Epoch:  32 Training Accuracy:  0.6549623393885688\n",
      "Epoch:  32 Val Accuracy:  0.5433125433125433\n",
      "Epoch:  33 Training Accuracy:  0.6583961010190519\n",
      "Epoch:  33 Val Accuracy:  0.5744975744975745\n",
      "Epoch:  34 Training Accuracy:  0.6695835179441737\n",
      "Epoch:  34 Val Accuracy:  0.568953568953569\n",
      "Epoch:  35 Training Accuracy:  0.6741249446167479\n",
      "Epoch:  35 Val Accuracy:  0.5703395703395704\n",
      "Epoch:  36 Training Accuracy:  0.6166371289322109\n",
      "Epoch:  36 Val Accuracy:  0.5821205821205822\n",
      "Epoch:  37 Training Accuracy:  0.6636021267168808\n",
      "Epoch:  37 Val Accuracy:  0.5592515592515592\n",
      "Epoch:  38 Training Accuracy:  0.6757864421798848\n",
      "Epoch:  38 Val Accuracy:  0.5911295911295912\n",
      "Epoch:  39 Training Accuracy:  0.6712450155073106\n",
      "Epoch:  39 Val Accuracy:  0.5793485793485793\n",
      "Epoch:  40 Training Accuracy:  0.6847585290208241\n",
      "Epoch:  40 Val Accuracy:  0.5897435897435898\n",
      "Epoch:  41 Training Accuracy:  0.6832077979618963\n",
      "Epoch:  41 Val Accuracy:  0.5987525987525988\n",
      "Epoch:  42 Training Accuracy:  0.6858661940629154\n",
      "Epoch:  42 Val Accuracy:  0.5862785862785863\n",
      "Epoch:  43 Training Accuracy:  0.6922906513070447\n",
      "Epoch:  43 Val Accuracy:  0.5793485793485793\n",
      "Epoch:  44 Training Accuracy:  0.6896322552060257\n",
      "Epoch:  44 Val Accuracy:  0.5994455994455995\n",
      "Epoch:  45 Training Accuracy:  0.6908506867523261\n",
      "Epoch:  45 Val Accuracy:  0.5731115731115731\n",
      "Epoch:  46 Training Accuracy:  0.6918475852902083\n",
      "Epoch:  46 Val Accuracy:  0.5682605682605683\n",
      "Epoch:  47 Training Accuracy:  0.6884138236597253\n",
      "Epoch:  47 Val Accuracy:  0.5855855855855856\n",
      "Epoch:  48 Training Accuracy:  0.7035888347363757\n",
      "Epoch:  48 Val Accuracy:  0.5807345807345807\n",
      "Epoch:  49 Training Accuracy:  0.6992689410722197\n",
      "Epoch:  49 Val Accuracy:  0.6098406098406098\n",
      "Epoch:  50 Training Accuracy:  0.6928444838280904\n",
      "Epoch:  50 Val Accuracy:  0.589050589050589\n",
      "Epoch:  51 Training Accuracy:  0.7056933983163491\n",
      "Epoch:  51 Val Accuracy:  0.5952875952875953\n",
      "Epoch:  52 Training Accuracy:  0.7018165706690297\n",
      "Epoch:  52 Val Accuracy:  0.5641025641025641\n",
      "Epoch:  0 Training Accuracy:  0.3359548072662827\n",
      "Epoch:  0 Val Accuracy:  0.27442827442827444\n",
      "Epoch:  1 Training Accuracy:  0.39488258750553834\n",
      "Epoch:  1 Val Accuracy:  0.21483021483021483\n",
      "Epoch:  2 Training Accuracy:  0.4147097917589721\n",
      "Epoch:  2 Val Accuracy:  0.2515592515592516\n",
      "Epoch:  3 Training Accuracy:  0.37250775365529465\n",
      "Epoch:  3 Val Accuracy:  0.26056826056826055\n",
      "Epoch:  4 Training Accuracy:  0.4099468320779796\n",
      "Epoch:  4 Val Accuracy:  0.2674982674982675\n",
      "Epoch:  5 Training Accuracy:  0.42080194949047406\n",
      "Epoch:  5 Val Accuracy:  0.3201663201663202\n",
      "Epoch:  6 Training Accuracy:  0.42844483828090385\n",
      "Epoch:  6 Val Accuracy:  0.2806652806652807\n",
      "Epoch:  7 Training Accuracy:  0.4844926894107222\n",
      "Epoch:  7 Val Accuracy:  0.27927927927927926\n",
      "Epoch:  8 Training Accuracy:  0.4898094816127603\n",
      "Epoch:  8 Val Accuracy:  0.2862092862092862\n",
      "Epoch:  9 Training Accuracy:  0.49047408063801506\n",
      "Epoch:  9 Val Accuracy:  0.3194733194733195\n",
      "Epoch:  10 Training Accuracy:  0.506978289765175\n",
      "Epoch:  10 Val Accuracy:  0.33264033264033266\n",
      "Epoch:  11 Training Accuracy:  0.525254762959681\n",
      "Epoch:  11 Val Accuracy:  0.38946638946638945\n",
      "Epoch:  12 Training Accuracy:  0.5395436420026584\n",
      "Epoch:  12 Val Accuracy:  0.43104643104643103\n",
      "Epoch:  13 Training Accuracy:  0.5535002215330084\n",
      "Epoch:  13 Val Accuracy:  0.4435204435204435\n",
      "Epoch:  14 Training Accuracy:  0.621953921134249\n",
      "Epoch:  14 Val Accuracy:  0.5162855162855163\n",
      "Epoch:  15 Training Accuracy:  0.6434426229508197\n",
      "Epoch:  15 Val Accuracy:  0.5280665280665281\n",
      "Epoch:  16 Training Accuracy:  0.6622729286663713\n",
      "Epoch:  16 Val Accuracy:  0.5765765765765766\n",
      "Epoch:  17 Training Accuracy:  0.6775587062472308\n",
      "Epoch:  17 Val Accuracy:  0.632016632016632\n",
      "Epoch:  18 Training Accuracy:  0.6809924678777137\n",
      "Epoch:  18 Val Accuracy:  0.6133056133056133\n",
      "Epoch:  19 Training Accuracy:  0.6990474080638015\n",
      "Epoch:  19 Val Accuracy:  0.5765765765765766\n",
      "Epoch:  20 Training Accuracy:  0.7368187859991139\n",
      "Epoch:  20 Val Accuracy:  0.6472626472626473\n",
      "Epoch:  21 Training Accuracy:  0.7487815684536996\n",
      "Epoch:  21 Val Accuracy:  0.6278586278586279\n",
      "Epoch:  22 Training Accuracy:  0.7475631369073992\n",
      "Epoch:  22 Val Accuracy:  0.6493416493416494\n",
      "Epoch:  23 Training Accuracy:  0.7657288435976961\n",
      "Epoch:  23 Val Accuracy:  0.6514206514206514\n",
      "Epoch:  24 Training Accuracy:  0.7586397873283119\n",
      "Epoch:  24 Val Accuracy:  0.6735966735966736\n",
      "Epoch:  25 Training Accuracy:  0.7748116969428445\n",
      "Epoch:  25 Val Accuracy:  0.6327096327096328\n",
      "Epoch:  26 Training Accuracy:  0.7877713779353124\n",
      "Epoch:  26 Val Accuracy:  0.6084546084546084\n",
      "Epoch:  27 Training Accuracy:  0.7913159060700045\n",
      "Epoch:  27 Val Accuracy:  0.6444906444906445\n",
      "Epoch:  28 Training Accuracy:  0.7995126273814799\n",
      "Epoch:  28 Val Accuracy:  0.6632016632016632\n",
      "Epoch:  29 Training Accuracy:  0.8173460345591493\n",
      "Epoch:  29 Val Accuracy:  0.6354816354816355\n",
      "Epoch:  30 Training Accuracy:  0.8091493132476739\n",
      "Epoch:  30 Val Accuracy:  0.6812196812196812\n",
      "Epoch:  31 Training Accuracy:  0.7123393885688968\n",
      "Epoch:  31 Val Accuracy:  0.5835065835065835\n",
      "Epoch:  32 Training Accuracy:  0.8091493132476739\n",
      "Epoch:  32 Val Accuracy:  0.6271656271656272\n",
      "Epoch:  33 Training Accuracy:  0.805715551617191\n",
      "Epoch:  33 Val Accuracy:  0.6327096327096328\n",
      "Epoch:  34 Training Accuracy:  0.8142445724412938\n",
      "Epoch:  34 Val Accuracy:  0.6548856548856549\n",
      "Epoch:  35 Training Accuracy:  0.833628710677891\n",
      "Epoch:  35 Val Accuracy:  0.6368676368676369\n",
      "Epoch:  36 Training Accuracy:  0.8321887461231724\n",
      "Epoch:  36 Val Accuracy:  0.6431046431046431\n",
      "Epoch:  37 Training Accuracy:  0.851019051838724\n",
      "Epoch:  37 Val Accuracy:  0.6521136521136521\n",
      "Epoch:  38 Training Accuracy:  0.83883473637572\n",
      "Epoch:  38 Val Accuracy:  0.6437976437976438\n",
      "Epoch:  39 Training Accuracy:  0.8525697828976517\n",
      "Epoch:  39 Val Accuracy:  0.6396396396396397\n",
      "Epoch:  40 Training Accuracy:  0.8779353123615419\n",
      "Epoch:  40 Val Accuracy:  0.6347886347886348\n",
      "Epoch:  41 Training Accuracy:  0.8833628710677891\n",
      "Epoch:  41 Val Accuracy:  0.632016632016632\n",
      "Epoch:  42 Training Accuracy:  0.8803721754541427\n",
      "Epoch:  42 Val Accuracy:  0.6562716562716563\n",
      "Epoch:  43 Training Accuracy:  0.8851351351351351\n",
      "Epoch:  43 Val Accuracy:  0.638946638946639\n",
      "Epoch:  44 Training Accuracy:  0.879818342933097\n",
      "Epoch:  44 Val Accuracy:  0.6382536382536382\n",
      "Epoch:  45 Training Accuracy:  0.8885688967656181\n",
      "Epoch:  45 Val Accuracy:  0.6285516285516286\n",
      "Epoch:  46 Training Accuracy:  0.8904519273371733\n",
      "Epoch:  46 Val Accuracy:  0.6458766458766458\n",
      "Epoch:  47 Training Accuracy:  0.8939964554718653\n",
      "Epoch:  47 Val Accuracy:  0.6417186417186417\n",
      "Epoch:  48 Training Accuracy:  0.9036331413380594\n",
      "Epoch:  48 Val Accuracy:  0.638946638946639\n",
      "Epoch:  49 Training Accuracy:  0.9034116083296411\n",
      "Epoch:  49 Val Accuracy:  0.6493416493416494\n",
      "Epoch:  50 Training Accuracy:  0.9011962782454586\n",
      "Epoch:  50 Val Accuracy:  0.6361746361746362\n",
      "Epoch:  51 Training Accuracy:  0.9030793088170137\n",
      "Epoch:  51 Val Accuracy:  0.6458766458766458\n",
      "Epoch:  52 Training Accuracy:  0.9087284005316792\n",
      "Epoch:  52 Val Accuracy:  0.6299376299376299\n",
      "Epoch:  53 Training Accuracy:  0.9088391670358883\n",
      "Epoch:  53 Val Accuracy:  0.6306306306306306\n",
      "Epoch:  54 Training Accuracy:  0.9161497563136908\n",
      "Epoch:  54 Val Accuracy:  0.6396396396396397\n",
      "Epoch:  55 Training Accuracy:  0.906734603455915\n",
      "Epoch:  55 Val Accuracy:  0.6451836451836452\n",
      "Epoch:  56 Training Accuracy:  0.9160389898094816\n",
      "Epoch:  56 Val Accuracy:  0.6375606375606375\n",
      "Epoch:  57 Training Accuracy:  0.9142667257421356\n",
      "Epoch:  57 Val Accuracy:  0.6465696465696466\n",
      "Epoch:  58 Training Accuracy:  0.9144882587505538\n",
      "Epoch:  58 Val Accuracy:  0.6541926541926542\n",
      "Epoch:  59 Training Accuracy:  0.9140451927337173\n",
      "Epoch:  59 Val Accuracy:  0.6618156618156619\n",
      "Epoch:  60 Training Accuracy:  0.9244572441293752\n",
      "Epoch:  60 Val Accuracy:  0.6632016632016632\n",
      "Epoch:  61 Training Accuracy:  0.926007975188303\n",
      "Epoch:  61 Val Accuracy:  0.6597366597366597\n",
      "Epoch:  62 Training Accuracy:  0.9247895436420026\n",
      "Epoch:  62 Val Accuracy:  0.6521136521136521\n",
      "Epoch:  63 Training Accuracy:  0.9270048737261852\n",
      "Epoch:  63 Val Accuracy:  0.65003465003465\n",
      "Epoch:  64 Training Accuracy:  0.9302171023482498\n",
      "Epoch:  64 Val Accuracy:  0.6514206514206514\n",
      "Epoch:  65 Training Accuracy:  0.9270048737261852\n",
      "Epoch:  65 Val Accuracy:  0.6521136521136521\n",
      "Epoch:  66 Training Accuracy:  0.9304386353566682\n",
      "Epoch:  66 Val Accuracy:  0.659043659043659\n",
      "Epoch:  67 Training Accuracy:  0.9318785999113868\n",
      "Epoch:  67 Val Accuracy:  0.6548856548856549\n",
      "Epoch:  68 Training Accuracy:  0.9326539654408507\n",
      "Epoch:  68 Val Accuracy:  0.6521136521136521\n",
      "Epoch:  69 Training Accuracy:  0.9340939299955694\n",
      "Epoch:  69 Val Accuracy:  0.6541926541926542\n",
      "Epoch:  70 Training Accuracy:  0.937195392113425\n",
      "Epoch:  70 Val Accuracy:  0.65003465003465\n",
      "Epoch:  71 Training Accuracy:  0.9359769605671245\n",
      "Epoch:  71 Val Accuracy:  0.6507276507276507\n",
      "Epoch:  72 Training Accuracy:  0.9360877270713336\n",
      "Epoch:  72 Val Accuracy:  0.6458766458766458\n",
      "Epoch:  0 Training Accuracy:  0.3142445724412938\n",
      "Epoch:  0 Val Accuracy:  0.2577962577962578\n",
      "Epoch:  1 Training Accuracy:  0.4253433761630483\n",
      "Epoch:  1 Val Accuracy:  0.23007623007623007\n",
      "Epoch:  2 Training Accuracy:  0.4455028799291094\n",
      "Epoch:  2 Val Accuracy:  0.3257103257103257\n",
      "Epoch:  3 Training Accuracy:  0.47518830305715554\n",
      "Epoch:  3 Val Accuracy:  0.2945252945252945\n",
      "Epoch:  4 Training Accuracy:  0.5440850686752327\n",
      "Epoch:  4 Val Accuracy:  0.3367983367983368\n",
      "Epoch:  5 Training Accuracy:  0.5894993354009748\n",
      "Epoch:  5 Val Accuracy:  0.49064449064449067\n",
      "Epoch:  6 Training Accuracy:  0.6240584847142224\n",
      "Epoch:  6 Val Accuracy:  0.6077616077616078\n",
      "Epoch:  7 Training Accuracy:  0.6655959237926451\n",
      "Epoch:  7 Val Accuracy:  0.5835065835065835\n",
      "Epoch:  8 Training Accuracy:  0.6761187416925122\n",
      "Epoch:  8 Val Accuracy:  0.5571725571725572\n",
      "Epoch:  9 Training Accuracy:  0.7005981391227293\n",
      "Epoch:  9 Val Accuracy:  0.546084546084546\n",
      "Epoch:  10 Training Accuracy:  0.7104563579973416\n",
      "Epoch:  10 Val Accuracy:  0.6160776160776161\n",
      "Epoch:  11 Training Accuracy:  0.7286220646876385\n",
      "Epoch:  11 Val Accuracy:  0.65003465003465\n",
      "Epoch:  12 Training Accuracy:  0.763070447496677\n",
      "Epoch:  12 Val Accuracy:  0.6742896742896742\n",
      "Epoch:  13 Training Accuracy:  0.7505538325210457\n",
      "Epoch:  13 Val Accuracy:  0.6437976437976438\n",
      "Epoch:  14 Training Accuracy:  0.7801284891448826\n",
      "Epoch:  14 Val Accuracy:  0.6687456687456688\n",
      "Epoch:  15 Training Accuracy:  0.784780682321666\n",
      "Epoch:  15 Val Accuracy:  0.5876645876645876\n",
      "Epoch:  16 Training Accuracy:  0.7885467434647763\n",
      "Epoch:  16 Val Accuracy:  0.6271656271656272\n",
      "Epoch:  17 Training Accuracy:  0.7990695613646434\n",
      "Epoch:  17 Val Accuracy:  0.6313236313236313\n",
      "Epoch:  18 Training Accuracy:  0.8115861763402747\n",
      "Epoch:  18 Val Accuracy:  0.6112266112266113\n",
      "Epoch:  19 Training Accuracy:  0.8330748781568453\n",
      "Epoch:  19 Val Accuracy:  0.6354816354816355\n",
      "Epoch:  20 Training Accuracy:  0.8987594151528577\n",
      "Epoch:  20 Val Accuracy:  0.6153846153846154\n",
      "Epoch:  21 Training Accuracy:  0.9134913602126717\n",
      "Epoch:  21 Val Accuracy:  0.6396396396396397\n",
      "Epoch:  22 Training Accuracy:  0.9262295081967213\n",
      "Epoch:  22 Val Accuracy:  0.6340956340956341\n",
      "Epoch:  23 Training Accuracy:  0.9390784226849801\n",
      "Epoch:  23 Val Accuracy:  0.6008316008316008\n",
      "Epoch:  24 Training Accuracy:  0.9501550731058928\n",
      "Epoch:  24 Val Accuracy:  0.6126126126126126\n",
      "Epoch:  25 Training Accuracy:  0.9461674789543641\n",
      "Epoch:  25 Val Accuracy:  0.632016632016632\n",
      "Epoch:  26 Training Accuracy:  0.9584625609215773\n",
      "Epoch:  26 Val Accuracy:  0.613998613998614\n",
      "Epoch:  27 Training Accuracy:  0.9623393885688968\n",
      "Epoch:  27 Val Accuracy:  0.638946638946639\n",
      "Epoch:  28 Training Accuracy:  0.9632255206025698\n",
      "Epoch:  28 Val Accuracy:  0.6146916146916147\n",
      "Epoch:  29 Training Accuracy:  0.9721976074435091\n",
      "Epoch:  29 Val Accuracy:  0.6077616077616078\n",
      "Epoch:  30 Training Accuracy:  0.967766947275144\n",
      "Epoch:  30 Val Accuracy:  0.6354816354816355\n",
      "Epoch:  31 Training Accuracy:  0.9623393885688968\n",
      "Epoch:  31 Val Accuracy:  0.6375606375606375\n",
      "Epoch:  32 Training Accuracy:  0.9771821001329198\n",
      "Epoch:  32 Val Accuracy:  0.6243936243936244\n",
      "Epoch:  33 Training Accuracy:  0.9792866637128932\n",
      "Epoch:  33 Val Accuracy:  0.638946638946639\n",
      "Epoch:  34 Training Accuracy:  0.9807266282676119\n",
      "Epoch:  34 Val Accuracy:  0.6278586278586279\n",
      "Epoch:  35 Training Accuracy:  0.9779574656623837\n",
      "Epoch:  35 Val Accuracy:  0.6534996534996536\n",
      "Epoch:  36 Training Accuracy:  0.9792866637128932\n",
      "Epoch:  36 Val Accuracy:  0.6313236313236313\n",
      "Epoch:  37 Training Accuracy:  0.9839388568896765\n",
      "Epoch:  37 Val Accuracy:  0.6424116424116424\n",
      "Epoch:  38 Training Accuracy:  0.9745237040319008\n",
      "Epoch:  38 Val Accuracy:  0.6292446292446292\n",
      "Epoch:  39 Training Accuracy:  0.9698715108551174\n",
      "Epoch:  39 Val Accuracy:  0.6403326403326404\n",
      "Epoch:  40 Training Accuracy:  0.9926894107221976\n",
      "Epoch:  40 Val Accuracy:  0.6604296604296604\n",
      "Epoch:  41 Training Accuracy:  0.994793974302171\n",
      "Epoch:  41 Val Accuracy:  0.6555786555786556\n",
      "Epoch:  42 Training Accuracy:  0.9953478068232167\n",
      "Epoch:  42 Val Accuracy:  0.6611226611226612\n",
      "Epoch:  43 Training Accuracy:  0.9952370403190075\n",
      "Epoch:  43 Val Accuracy:  0.6597366597366597\n",
      "Epoch:  44 Training Accuracy:  0.995680106335844\n",
      "Epoch:  44 Val Accuracy:  0.6534996534996536\n",
      "Epoch:  45 Training Accuracy:  0.9959016393442623\n",
      "Epoch:  45 Val Accuracy:  0.6424116424116424\n",
      "Epoch:  46 Training Accuracy:  0.9976739034116083\n",
      "Epoch:  46 Val Accuracy:  0.6458766458766458\n",
      "Epoch:  47 Training Accuracy:  0.9975631369073992\n",
      "Epoch:  47 Val Accuracy:  0.6493416493416494\n",
      "Epoch:  48 Training Accuracy:  0.9971200708905626\n",
      "Epoch:  48 Val Accuracy:  0.6437976437976438\n",
      "Epoch:  49 Training Accuracy:  0.9972308373947718\n",
      "Epoch:  49 Val Accuracy:  0.6327096327096328\n",
      "Epoch:  50 Training Accuracy:  0.9957908728400532\n",
      "Epoch:  50 Val Accuracy:  0.6451836451836452\n",
      "Epoch:  51 Training Accuracy:  0.9984492689410722\n",
      "Epoch:  51 Val Accuracy:  0.6514206514206514\n",
      "Epoch:  52 Training Accuracy:  0.9983385024368631\n",
      "Epoch:  52 Val Accuracy:  0.6410256410256411\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,(x,y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train):\n\u001b[1;32m     23\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 24\u001b[0m     y_pred\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss_fn(y_pred,y)\n\u001b[1;32m     26\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/memeye_studies/model.py:24\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_dim, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim,device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mrequires_grad_()\n\u001b[1;32m     23\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_dim, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim,device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mrequires_grad_()\n\u001b[0;32m---> 24\u001b[0m out, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m out \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     26\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_classes=6\n",
    "dropout=0.1\n",
    "for second in [3,2.5,2,1.5]:\n",
    "    train=torch.load('train'+str(second)+'_dataloader.pt')\n",
    "    val=torch.load('val'+str(second)+'_dataloader.pt')\n",
    "    for layer in [3,4,2]:\n",
    "            for hidden_dim in [16,32,64]:\n",
    "                torch.manual_seed(0)\n",
    "                model=LSTMModel(11,hidden_dim,layer,output_classes,dropout)\n",
    "                optimizer=torch.optim.AdamW(model.parameters(),lr=0.01)\n",
    "                scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=20,gamma=0.3)\n",
    "                filename='model'+str(second)+'/'+str(layer)+'_'+str(hidden_dim)\n",
    "                if not os.path.exists(filename):\n",
    "                    os.makedirs(filename)\n",
    "                f=open(filename+'/results.txt','w')\n",
    "                loss_arr=[]\n",
    "                training_accuracy=[]\n",
    "                val_accuracy=[]\n",
    "                for epoch in range(100):\n",
    "                    model.train()\n",
    "                    loss_sum=0\n",
    "                    for i,(x,y) in enumerate(train):\n",
    "                        optimizer.zero_grad()\n",
    "                        y_pred=model(x)\n",
    "                        loss=loss_fn(y_pred,y)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        loss_sum+=loss.item()\n",
    "                    scheduler.step()\n",
    "                    loss_sum/=len(train)\n",
    "                    loss_arr.append(loss_sum)\n",
    "                    model.eval()\n",
    "                    correct=0\n",
    "                    total=0\n",
    "                    #Calculate accuracy per class \n",
    "                    accuracy_per_class=[0]*output_classes\n",
    "                    fp_per_class=[0]*output_classes\n",
    "                    fn_per_class=[0]*output_classes\n",
    "                    tp_per_class=[0]*output_classes\n",
    "                    tn_per_class=[0]*output_classes\n",
    "                    fp_rate_per_class=[0]*output_classes\n",
    "                    for i,(x,y) in enumerate(train):\n",
    "                        y_pred=model(x)\n",
    "                        _,predicted=torch.max(y_pred.data,1)\n",
    "                        total+=y.size(0)\n",
    "                        y=torch.max(y,1)[1]\n",
    "                        correct+=(predicted==y).sum().item()\n",
    "                        for i in range(len(y)):\n",
    "                            if y[i]==predicted[i]:\n",
    "                                tp_per_class[y[i]]+=1\n",
    "                                for j in range(output_classes):\n",
    "                                    if j!=y[i]:\n",
    "                                        tn_per_class[j]+=1\n",
    "                            else:\n",
    "                                fp_per_class[predicted[i]]+=1\n",
    "                                fn_per_class[y[i]]+=1\n",
    "                                for j in range(output_classes):\n",
    "                                    if j!=y[i] and j!=predicted[i]:\n",
    "                                        tn_per_class[j]+=1\n",
    "                    for i in range(output_classes):\n",
    "                        fp_rate_per_class[i]=fp_per_class[i]/(fp_per_class[i]+tn_per_class[i])\n",
    "                        accuracy_per_class[i]=tp_per_class[i]/(tp_per_class[i]+fn_per_class[i])\n",
    "                    print('Epoch: ',epoch,'Training Accuracy: ',correct/total)\n",
    "                    training_accuracy.append(correct/total)\n",
    "                    f.write('Epoch: '+str(epoch)+' Training Accuracy: '+str(correct/total)+'\\n')\n",
    "                    f.write('Accuracy per class: '+str(accuracy_per_class)+'\\n')\n",
    "                    f.write('False positive rate per class: '+str(fp_rate_per_class)+'\\n')\n",
    "                    f.write('False positive per class: '+str(fp_per_class)+'\\n')\n",
    "                    f.write('False negative per class: '+str(fn_per_class)+'\\n')\n",
    "                    f.write('True positive per class: '+str(tp_per_class)+'\\n')\n",
    "                    f.write('True negative per class: '+str(tn_per_class)+'\\n')\n",
    "                    f.write('\\n')\n",
    "                    correct=0\n",
    "                    total=0\n",
    "                    accuracy_per_class=[0]*output_classes\n",
    "                    fp_per_class=[0]*output_classes\n",
    "                    fn_per_class=[0]*output_classes\n",
    "                    tp_per_class=[0]*output_classes\n",
    "                    tn_per_class=[0]*output_classes\n",
    "                    fp_rate_per_class=[0]*output_classes\n",
    "                    for i,(x,y) in enumerate(val):\n",
    "                        y_pred=model(x)\n",
    "                        _,predicted=torch.max(y_pred.data,1)\n",
    "                        total+=y.size(0)\n",
    "                        y=torch.max(y,1)[1]\n",
    "                        correct+=(predicted==y).sum().item()\n",
    "                        for i in range(len(y)):\n",
    "                            if y[i]==predicted[i]:\n",
    "                                tp_per_class[y[i]]+=1\n",
    "                                for j in range(output_classes):\n",
    "                                    if j!=y[i]:\n",
    "                                        tn_per_class[j]+=1\n",
    "                            else:\n",
    "                                fp_per_class[predicted[i]]+=1\n",
    "                                fn_per_class[y[i]]+=1\n",
    "                                for j in range(output_classes):\n",
    "                                    if j!=y[i] and j!=predicted[i]:\n",
    "                                        tn_per_class[j]+=1\n",
    "                    for i in range(output_classes):\n",
    "                        fp_rate_per_class[i]=fp_per_class[i]/(fp_per_class[i]+tn_per_class[i])\n",
    "                        if tp_per_class[i]+fn_per_class[i]==0:\n",
    "                            accuracy_per_class[i]=1\n",
    "                        else:\n",
    "                            accuracy_per_class[i]=tp_per_class[i]/(tp_per_class[i]+fn_per_class[i])\n",
    "                    print('Epoch: ',epoch,'Val Accuracy: ',correct/total)\n",
    "                    val_accuracy.append(correct/total)\n",
    "                    f.write('Epoch: '+str(epoch)+' Val Accuracy: '+str(correct/total)+'\\n')\n",
    "                    f.write('Accuracy per class: '+str(accuracy_per_class)+'\\n')\n",
    "                    f.write('False positive rate per class: '+str(fp_rate_per_class)+'\\n')\n",
    "                    f.write('False positive per class: '+str(fp_per_class)+'\\n')\n",
    "                    f.write('False negative per class: '+str(fn_per_class)+'\\n')\n",
    "                    f.write('True positive per class: '+str(tp_per_class)+'\\n')\n",
    "                    f.write('True negative per class: '+str(tn_per_class)+'\\n')\n",
    "                    f.write('\\n')\n",
    "                    torch.save(model,filename+'/model'+str(epoch)+'.pt') \n",
    "                    #Early stopping\n",
    "                    if epoch>50 and val_accuracy[-1]<val_accuracy[-2] and training_accuracy[-1]-training_accuracy[-10]<0.01:\n",
    "                        break  \n",
    "                f.write('Best val accuracy: '+str(max(val_accuracy))+'\\n')\n",
    "                f.write('Epoch with best val accuracy: '+str(val_accuracy.index(max(val_accuracy)))+'\\n')\n",
    "                plt.plot(loss_arr)\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.savefig(filename+'/loss.png')\n",
    "                plt.close()\n",
    "                plt.plot(training_accuracy)\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Training Accuracy')\n",
    "                plt.savefig(filename+'/training_accuracy.png')\n",
    "                plt.close()\n",
    "                plt.plot(val_accuracy)\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Validation Accuracy')\n",
    "                plt.savefig(filename+'/val_accuracy.png')\n",
    "                f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

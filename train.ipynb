{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Dataset\n",
    "import torch\n",
    "from model import LSTMModel\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=torch.load('train4_3_dataloader.pt')\n",
    "val=torch.load('val4_3_dataloader.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Training Accuracy:  0.3491610818004658\n",
      "Epoch:  0 Val Accuracy:  0.34395198522622344\n",
      "Epoch:  1 Training Accuracy:  0.4726460383098056\n",
      "Epoch:  1 Val Accuracy:  0.4090489381348107\n",
      "Epoch:  2 Training Accuracy:  0.4709824611435905\n",
      "Epoch:  2 Val Accuracy:  0.42382271468144045\n",
      "Epoch:  3 Training Accuracy:  0.4907077332572841\n",
      "Epoch:  3 Val Accuracy:  0.40812557710064634\n",
      "Epoch:  4 Training Accuracy:  0.49493797233708825\n",
      "Epoch:  4 Val Accuracy:  0.42566943674976915\n",
      "Epoch:  5 Training Accuracy:  0.5164218831693521\n",
      "Epoch:  5 Val Accuracy:  0.415050784856879\n",
      "Epoch:  6 Training Accuracy:  0.4956984647559295\n",
      "Epoch:  6 Val Accuracy:  0.41920590951061865\n",
      "Epoch:  7 Training Accuracy:  0.5048719045582014\n",
      "Epoch:  7 Val Accuracy:  0.4196675900277008\n",
      "Epoch:  8 Training Accuracy:  0.5127620134036789\n",
      "Epoch:  8 Val Accuracy:  0.4242843951985226\n",
      "Epoch:  9 Training Accuracy:  0.5156138599743334\n",
      "Epoch:  9 Val Accuracy:  0.402123730378578\n",
      "Epoch:  10 Training Accuracy:  0.5246922382242502\n",
      "Epoch:  10 Val Accuracy:  0.43536472760849493\n",
      "Epoch:  11 Training Accuracy:  0.520794714577689\n",
      "Epoch:  11 Val Accuracy:  0.41412742382271467\n",
      "Epoch:  12 Training Accuracy:  0.5558248966205618\n",
      "Epoch:  12 Val Accuracy:  0.44136657433056325\n",
      "Epoch:  13 Training Accuracy:  0.5720328912971149\n",
      "Epoch:  13 Val Accuracy:  0.45106186518928904\n",
      "Epoch:  14 Training Accuracy:  0.5746470839868815\n",
      "Epoch:  14 Val Accuracy:  0.47368421052631576\n",
      "Epoch:  15 Training Accuracy:  0.5745520224345264\n",
      "Epoch:  15 Val Accuracy:  0.5041551246537396\n",
      "Epoch:  16 Training Accuracy:  0.5913779172013879\n",
      "Epoch:  16 Val Accuracy:  0.4986149584487535\n",
      "Epoch:  17 Training Accuracy:  0.6219877370597462\n",
      "Epoch:  17 Val Accuracy:  0.5475530932594644\n",
      "Epoch:  18 Training Accuracy:  0.6307809306525975\n",
      "Epoch:  18 Val Accuracy:  0.5946445060018467\n",
      "Epoch:  19 Training Accuracy:  0.6560673035790674\n",
      "Epoch:  19 Val Accuracy:  0.6126500461680517\n",
      "Epoch:  20 Training Accuracy:  0.7016017871571842\n",
      "Epoch:  20 Val Accuracy:  0.6380424746075716\n",
      "Epoch:  21 Training Accuracy:  0.713817196634821\n",
      "Epoch:  21 Val Accuracy:  0.6717451523545707\n",
      "Epoch:  22 Training Accuracy:  0.7212795284947003\n",
      "Epoch:  22 Val Accuracy:  0.6768236380424746\n",
      "Epoch:  23 Training Accuracy:  0.7179048433860925\n",
      "Epoch:  23 Val Accuracy:  0.7179132040627886\n",
      "Epoch:  24 Training Accuracy:  0.7434288701934503\n",
      "Epoch:  24 Val Accuracy:  0.7197599261311173\n",
      "Epoch:  25 Training Accuracy:  0.7491325633347593\n",
      "Epoch:  25 Val Accuracy:  0.7183748845798708\n",
      "Epoch:  26 Training Accuracy:  0.759684395646181\n",
      "Epoch:  26 Val Accuracy:  0.760387811634349\n",
      "Epoch:  27 Training Accuracy:  0.7747041209182945\n",
      "Epoch:  27 Val Accuracy:  0.7613111726685133\n",
      "Epoch:  28 Training Accuracy:  0.7712343742573317\n",
      "Epoch:  28 Val Accuracy:  0.7793167128347184\n",
      "Epoch:  29 Training Accuracy:  0.7759874518750891\n",
      "Epoch:  29 Val Accuracy:  0.7751615881809788\n",
      "Epoch:  30 Training Accuracy:  0.7866818765150435\n",
      "Epoch:  30 Val Accuracy:  0.7876269621421976\n",
      "Epoch:  31 Training Accuracy:  0.7662436427586863\n",
      "Epoch:  31 Val Accuracy:  0.7797783933518005\n",
      "Epoch:  32 Training Accuracy:  0.7914824849089785\n",
      "Epoch:  32 Val Accuracy:  0.801477377654663\n",
      "Epoch:  33 Training Accuracy:  0.7926232235372404\n",
      "Epoch:  33 Val Accuracy:  0.8125577100646353\n",
      "Epoch:  34 Training Accuracy:  0.794904700793764\n",
      "Epoch:  34 Val Accuracy:  0.8180978762696214\n",
      "Epoch:  35 Training Accuracy:  0.7942868007034555\n",
      "Epoch:  35 Val Accuracy:  0.8180978762696214\n",
      "Epoch:  36 Training Accuracy:  0.8015590094586245\n",
      "Epoch:  36 Val Accuracy:  0.814404432132964\n",
      "Epoch:  37 Training Accuracy:  0.8052664100004753\n",
      "Epoch:  37 Val Accuracy:  0.814404432132964\n",
      "Epoch:  38 Training Accuracy:  0.804410856029279\n",
      "Epoch:  38 Val Accuracy:  0.8120960295475531\n",
      "Epoch:  39 Training Accuracy:  0.8032701174010172\n",
      "Epoch:  39 Val Accuracy:  0.8139427516158818\n",
      "Epoch:  40 Training Accuracy:  0.810114549170588\n",
      "Epoch:  40 Val Accuracy:  0.8222530009233611\n",
      "Epoch:  41 Training Accuracy:  0.8092589951993916\n",
      "Epoch:  41 Val Accuracy:  0.8264081255771006\n",
      "Epoch:  42 Training Accuracy:  0.8127287418603546\n",
      "Epoch:  42 Val Accuracy:  0.8287165281625115\n",
      "Epoch:  43 Training Accuracy:  0.8059318408669613\n",
      "Epoch:  43 Val Accuracy:  0.827331486611265\n",
      "Epoch:  44 Training Accuracy:  0.812348495650934\n",
      "Epoch:  44 Val Accuracy:  0.8291782086795937\n",
      "Epoch:  45 Training Accuracy:  0.8095441798564571\n",
      "Epoch:  45 Val Accuracy:  0.8194829178208679\n",
      "Epoch:  46 Training Accuracy:  0.8095917106326346\n",
      "Epoch:  46 Val Accuracy:  0.8310249307479224\n",
      "Epoch:  47 Training Accuracy:  0.8123009648747564\n",
      "Epoch:  47 Val Accuracy:  0.8356417359187442\n",
      "Epoch:  48 Training Accuracy:  0.810779980037074\n",
      "Epoch:  48 Val Accuracy:  0.8310249307479224\n",
      "Epoch:  49 Training Accuracy:  0.8107324492608964\n",
      "Epoch:  49 Val Accuracy:  0.8319482917820868\n",
      "Epoch:  50 Training Accuracy:  0.8187651504349066\n",
      "Epoch:  50 Val Accuracy:  0.8204062788550324\n",
      "Epoch:  51 Training Accuracy:  0.8144873805789249\n",
      "Epoch:  51 Val Accuracy:  0.8310249307479224\n",
      "Epoch:  52 Training Accuracy:  0.8093540567517468\n",
      "Epoch:  52 Val Accuracy:  0.832409972299169\n",
      "Epoch:  53 Training Accuracy:  0.8135842958315509\n",
      "Epoch:  53 Val Accuracy:  0.8305632502308402\n",
      "Epoch:  54 Training Accuracy:  0.8169114501639811\n",
      "Epoch:  54 Val Accuracy:  0.8351800554016621\n",
      "Epoch:  55 Training Accuracy:  0.8089738105423262\n",
      "Epoch:  55 Val Accuracy:  0.8277931671283472\n",
      "Epoch:  56 Training Accuracy:  0.8107324492608964\n",
      "Epoch:  56 Val Accuracy:  0.8356417359187442\n",
      "Epoch:  57 Training Accuracy:  0.8126336803079994\n",
      "Epoch:  57 Val Accuracy:  0.827331486611265\n",
      "Epoch:  58 Training Accuracy:  0.8216645277817387\n",
      "Epoch:  58 Val Accuracy:  0.8277931671283472\n",
      "Epoch:  59 Training Accuracy:  0.8052664100004753\n",
      "Epoch:  59 Val Accuracy:  0.8305632502308402\n",
      "Epoch:  60 Training Accuracy:  0.8158182423118969\n",
      "Epoch:  60 Val Accuracy:  0.8379501385041551\n",
      "Epoch:  61 Training Accuracy:  0.8172916963734018\n",
      "Epoch:  61 Val Accuracy:  0.8361034164358264\n",
      "Epoch:  62 Training Accuracy:  0.8182898426731309\n",
      "Epoch:  62 Val Accuracy:  0.8342566943674977\n",
      "Epoch:  63 Training Accuracy:  0.8192879889728599\n",
      "Epoch:  63 Val Accuracy:  0.8305632502308402\n",
      "Epoch:  64 Training Accuracy:  0.8210466276914302\n",
      "Epoch:  64 Val Accuracy:  0.832409972299169\n",
      "Epoch:  65 Training Accuracy:  0.8219497124388041\n",
      "Epoch:  65 Val Accuracy:  0.8328716528162512\n",
      "Epoch:  66 Training Accuracy:  0.8212367507961404\n",
      "Epoch:  66 Val Accuracy:  0.832409972299169\n",
      "Epoch:  67 Training Accuracy:  0.8185750273301963\n",
      "Epoch:  67 Val Accuracy:  0.8347183748845799\n",
      "Epoch:  68 Training Accuracy:  0.8200484813917012\n",
      "Epoch:  68 Val Accuracy:  0.8328716528162512\n",
      "Epoch:  69 Training Accuracy:  0.8207139122581871\n",
      "Epoch:  69 Val Accuracy:  0.8310249307479224\n",
      "Epoch:  70 Training Accuracy:  0.8217120585579163\n",
      "Epoch:  70 Val Accuracy:  0.8310249307479224\n",
      "Epoch:  71 Training Accuracy:  0.8172916963734018\n",
      "Epoch:  71 Val Accuracy:  0.8356417359187442\n",
      "Epoch:  72 Training Accuracy:  0.8205237891534769\n",
      "Epoch:  72 Val Accuracy:  0.8328716528162512\n",
      "Epoch:  73 Training Accuracy:  0.8181472503445981\n",
      "Epoch:  73 Val Accuracy:  0.8328716528162512\n",
      "Epoch:  74 Training Accuracy:  0.8172916963734018\n",
      "Epoch:  74 Val Accuracy:  0.8328716528162512\n",
      "Epoch:  75 Training Accuracy:  0.8209040353628975\n",
      "Epoch:  75 Val Accuracy:  0.8370267774699908\n",
      "Epoch:  76 Training Accuracy:  0.8210941584676078\n",
      "Epoch:  76 Val Accuracy:  0.8296398891966759\n",
      "Epoch:  77 Training Accuracy:  0.8204287276011217\n",
      "Epoch:  77 Val Accuracy:  0.8328716528162512\n",
      "Epoch:  78 Training Accuracy:  0.8192879889728599\n",
      "Epoch:  78 Val Accuracy:  0.8328716528162512\n",
      "Epoch:  79 Training Accuracy:  0.8209990969152526\n",
      "Epoch:  79 Val Accuracy:  0.8310249307479224\n",
      "Epoch:  80 Training Accuracy:  0.8209040353628975\n",
      "Epoch:  80 Val Accuracy:  0.8337950138504155\n",
      "Epoch:  81 Training Accuracy:  0.822520081752935\n",
      "Epoch:  81 Val Accuracy:  0.8305632502308402\n",
      "Epoch:  82 Training Accuracy:  0.8226626740814678\n",
      "Epoch:  82 Val Accuracy:  0.8342566943674977\n",
      "Epoch:  83 Training Accuracy:  0.8218546508864489\n",
      "Epoch:  83 Val Accuracy:  0.8361034164358264\n",
      "Epoch:  84 Training Accuracy:  0.8219021816626265\n",
      "Epoch:  84 Val Accuracy:  0.832409972299169\n",
      "Epoch:  85 Training Accuracy:  0.8216645277817387\n",
      "Epoch:  85 Val Accuracy:  0.8342566943674977\n",
      "Epoch:  86 Training Accuracy:  0.8224725509767574\n",
      "Epoch:  86 Val Accuracy:  0.8328716528162512\n",
      "Epoch:  87 Training Accuracy:  0.8217120585579163\n",
      "Epoch:  87 Val Accuracy:  0.8319482917820868\n",
      "Epoch:  88 Training Accuracy:  0.8207139122581871\n",
      "Epoch:  88 Val Accuracy:  0.8370267774699908\n",
      "Epoch:  89 Training Accuracy:  0.8213318123484956\n",
      "Epoch:  89 Val Accuracy:  0.8356417359187442\n",
      "Epoch:  90 Training Accuracy:  0.822187366319692\n",
      "Epoch:  90 Val Accuracy:  0.8351800554016621\n",
      "Epoch:  91 Training Accuracy:  0.8214744046770284\n",
      "Epoch:  91 Val Accuracy:  0.8374884579870729\n",
      "Epoch:  92 Training Accuracy:  0.8216645277817387\n",
      "Epoch:  92 Val Accuracy:  0.8337950138504155\n",
      "Epoch:  93 Training Accuracy:  0.821189220019963\n",
      "Epoch:  93 Val Accuracy:  0.8333333333333334\n",
      "Epoch:  94 Training Accuracy:  0.8222824278720472\n",
      "Epoch:  94 Val Accuracy:  0.8337950138504155\n",
      "Epoch:  95 Training Accuracy:  0.8218071201102713\n",
      "Epoch:  95 Val Accuracy:  0.8342566943674977\n",
      "Epoch:  96 Training Accuracy:  0.8231855126194211\n",
      "Epoch:  96 Val Accuracy:  0.832409972299169\n",
      "Epoch:  97 Training Accuracy:  0.8218071201102713\n",
      "Epoch:  97 Val Accuracy:  0.8365650969529086\n",
      "Epoch:  98 Training Accuracy:  0.8212367507961404\n",
      "Epoch:  98 Val Accuracy:  0.8370267774699908\n",
      "Epoch:  99 Training Accuracy:  0.8227577356338229\n",
      "Epoch:  99 Val Accuracy:  0.8365650969529086\n",
      "Epoch:  0 Training Accuracy:  0.4167023147487999\n",
      "Epoch:  0 Val Accuracy:  0.43028624192059095\n",
      "Epoch:  1 Training Accuracy:  0.4177955226008841\n",
      "Epoch:  1 Val Accuracy:  0.40120036934441367\n",
      "Epoch:  2 Training Accuracy:  0.4100955368601169\n",
      "Epoch:  2 Val Accuracy:  0.4048938134810711\n",
      "Epoch:  3 Training Accuracy:  0.44892818099719567\n",
      "Epoch:  3 Val Accuracy:  0.4312096029547553\n",
      "Epoch:  4 Training Accuracy:  0.4802509624982176\n",
      "Epoch:  4 Val Accuracy:  0.42105263157894735\n",
      "Epoch:  5 Training Accuracy:  0.4838157707115357\n",
      "Epoch:  5 Val Accuracy:  0.40766389658356417\n",
      "Epoch:  6 Training Accuracy:  0.5142829982413613\n",
      "Epoch:  6 Val Accuracy:  0.420129270544783\n",
      "Epoch:  7 Training Accuracy:  0.5100052283853795\n",
      "Epoch:  7 Val Accuracy:  0.41274238227146814\n",
      "Epoch:  8 Training Accuracy:  0.5230761918342126\n",
      "Epoch:  8 Val Accuracy:  0.4515235457063712\n",
      "Epoch:  9 Training Accuracy:  0.4118541755786872\n",
      "Epoch:  9 Val Accuracy:  0.3739612188365651\n",
      "Epoch:  10 Training Accuracy:  0.5163268216169971\n",
      "Epoch:  10 Val Accuracy:  0.4182825484764543\n",
      "Epoch:  11 Training Accuracy:  0.5392366557345881\n",
      "Epoch:  11 Val Accuracy:  0.438134810710988\n",
      "Epoch:  12 Training Accuracy:  0.5419934407528875\n",
      "Epoch:  12 Val Accuracy:  0.47229916897506924\n",
      "Epoch:  13 Training Accuracy:  0.558486620086506\n",
      "Epoch:  13 Val Accuracy:  0.4856879039704525\n",
      "Epoch:  14 Training Accuracy:  0.571462521982984\n",
      "Epoch:  14 Val Accuracy:  0.49722991689750695\n",
      "Epoch:  15 Training Accuracy:  0.5875279243310043\n",
      "Epoch:  15 Val Accuracy:  0.5166204986149584\n",
      "Epoch:  16 Training Accuracy:  0.6121013356148106\n",
      "Epoch:  16 Val Accuracy:  0.5623268698060941\n",
      "Epoch:  17 Training Accuracy:  0.6548315033984505\n",
      "Epoch:  17 Val Accuracy:  0.6223453370267775\n",
      "Epoch:  18 Training Accuracy:  0.6767907219924901\n",
      "Epoch:  18 Val Accuracy:  0.6694367497691598\n",
      "Epoch:  19 Training Accuracy:  0.6942345168496602\n",
      "Epoch:  19 Val Accuracy:  0.6906740535549399\n",
      "Epoch:  20 Training Accuracy:  0.7284091449213366\n",
      "Epoch:  20 Val Accuracy:  0.7428439519852262\n",
      "Epoch:  21 Training Accuracy:  0.7528399638766101\n",
      "Epoch:  21 Val Accuracy:  0.7728531855955678\n",
      "Epoch:  22 Training Accuracy:  0.767954750701079\n",
      "Epoch:  22 Val Accuracy:  0.7922437673130194\n",
      "Epoch:  23 Training Accuracy:  0.7842578069299871\n",
      "Epoch:  23 Val Accuracy:  0.8088642659279779\n",
      "Epoch:  24 Training Accuracy:  0.79870716288797\n",
      "Epoch:  24 Val Accuracy:  0.8153277931671283\n",
      "Epoch:  25 Training Accuracy:  0.8059318408669613\n",
      "Epoch:  25 Val Accuracy:  0.8254847645429363\n",
      "Epoch:  26 Training Accuracy:  0.8039355482675032\n",
      "Epoch:  26 Val Accuracy:  0.830101569713758\n",
      "Epoch:  27 Training Accuracy:  0.8172916963734018\n",
      "Epoch:  27 Val Accuracy:  0.832409972299169\n",
      "Epoch:  28 Training Accuracy:  0.8162460192974951\n",
      "Epoch:  28 Val Accuracy:  0.8388734995383195\n",
      "Epoch:  29 Training Accuracy:  0.8135367650553733\n",
      "Epoch:  29 Val Accuracy:  0.8282548476454293\n",
      "Epoch:  30 Training Accuracy:  0.8226626740814678\n",
      "Epoch:  30 Val Accuracy:  0.8287165281625115\n",
      "Epoch:  31 Training Accuracy:  0.8294120442986834\n",
      "Epoch:  31 Val Accuracy:  0.8314866112650047\n",
      "Epoch:  32 Training Accuracy:  0.8314083368981415\n",
      "Epoch:  32 Val Accuracy:  0.832409972299169\n",
      "Epoch:  33 Training Accuracy:  0.8220447739911593\n",
      "Epoch:  33 Val Accuracy:  0.8337950138504155\n",
      "Epoch:  34 Training Accuracy:  0.8335472218261324\n",
      "Epoch:  34 Val Accuracy:  0.8421052631578947\n",
      "Epoch:  35 Training Accuracy:  0.8274632824754028\n",
      "Epoch:  35 Val Accuracy:  0.8430286241920592\n",
      "Epoch:  36 Training Accuracy:  0.827558344027758\n",
      "Epoch:  36 Val Accuracy:  0.8259464450600185\n",
      "Epoch:  37 Training Accuracy:  0.8378249916821142\n",
      "Epoch:  37 Val Accuracy:  0.84533702677747\n",
      "Epoch:  38 Training Accuracy:  0.8364465991729645\n",
      "Epoch:  38 Val Accuracy:  0.8434903047091413\n",
      "Epoch:  39 Training Accuracy:  0.8422453538666287\n",
      "Epoch:  39 Val Accuracy:  0.8499538319482918\n",
      "Epoch:  40 Training Accuracy:  0.8456200389752365\n",
      "Epoch:  40 Val Accuracy:  0.8481071098799631\n",
      "Epoch:  41 Training Accuracy:  0.8490422548600218\n",
      "Epoch:  41 Val Accuracy:  0.8444136657433057\n",
      "Epoch:  42 Training Accuracy:  0.8516564475497885\n",
      "Epoch:  42 Val Accuracy:  0.8494921514312096\n",
      "Epoch:  43 Training Accuracy:  0.8494225010694425\n",
      "Epoch:  43 Val Accuracy:  0.8522622345337026\n",
      "Epoch:  44 Training Accuracy:  0.8517990398783212\n",
      "Epoch:  44 Val Accuracy:  0.8430286241920592\n",
      "Epoch:  45 Training Accuracy:  0.8529873092827606\n",
      "Epoch:  45 Val Accuracy:  0.8508771929824561\n",
      "Epoch:  46 Training Accuracy:  0.8527496554018728\n",
      "Epoch:  46 Val Accuracy:  0.8462603878116344\n",
      "Epoch:  47 Training Accuracy:  0.8484243547697133\n",
      "Epoch:  47 Val Accuracy:  0.8471837488457987\n",
      "Epoch:  48 Training Accuracy:  0.8540805171348448\n",
      "Epoch:  48 Val Accuracy:  0.853185595567867\n",
      "Epoch:  49 Training Accuracy:  0.8531299016112933\n",
      "Epoch:  49 Val Accuracy:  0.8462603878116344\n",
      "Epoch:  50 Training Accuracy:  0.8533200247160037\n",
      "Epoch:  50 Val Accuracy:  0.8536472760849492\n",
      "Epoch:  51 Training Accuracy:  0.8553163173154618\n",
      "Epoch:  51 Val Accuracy:  0.8485687903970452\n",
      "Epoch:  52 Training Accuracy:  0.8505632396977043\n",
      "Epoch:  52 Val Accuracy:  0.850415512465374\n",
      "Epoch:  53 Training Accuracy:  0.8572175483625648\n",
      "Epoch:  53 Val Accuracy:  0.8481071098799631\n",
      "Epoch:  54 Training Accuracy:  0.8517990398783212\n",
      "Epoch:  54 Val Accuracy:  0.8513388734995383\n",
      "Epoch:  55 Training Accuracy:  0.8561243405104806\n",
      "Epoch:  55 Val Accuracy:  0.8471837488457987\n",
      "Epoch:  56 Training Accuracy:  0.8569323637054993\n",
      "Epoch:  56 Val Accuracy:  0.8444136657433057\n",
      "Epoch:  57 Training Accuracy:  0.8541755786872\n",
      "Epoch:  57 Val Accuracy:  0.8508771929824561\n",
      "Epoch:  58 Training Accuracy:  0.8570274252578545\n",
      "Epoch:  58 Val Accuracy:  0.8536472760849492\n",
      "Epoch:  59 Training Accuracy:  0.8591187794096677\n",
      "Epoch:  59 Val Accuracy:  0.8430286241920592\n",
      "Epoch:  60 Training Accuracy:  0.8630163030562289\n",
      "Epoch:  60 Val Accuracy:  0.8457987072945522\n",
      "Epoch:  61 Training Accuracy:  0.8605447026949951\n",
      "Epoch:  61 Val Accuracy:  0.8476454293628809\n",
      "Epoch:  62 Training Accuracy:  0.8633965492656495\n",
      "Epoch:  62 Val Accuracy:  0.8457987072945522\n",
      "Epoch:  63 Training Accuracy:  0.8618280336517895\n",
      "Epoch:  63 Val Accuracy:  0.8457987072945522\n",
      "Epoch:  64 Training Accuracy:  0.8633965492656495\n",
      "Epoch:  64 Val Accuracy:  0.8481071098799631\n",
      "Epoch:  65 Training Accuracy:  0.8625885260706307\n",
      "Epoch:  65 Val Accuracy:  0.8457987072945522\n",
      "Epoch:  66 Training Accuracy:  0.8635866723703598\n",
      "Epoch:  66 Val Accuracy:  0.8476454293628809\n",
      "Epoch:  67 Training Accuracy:  0.8652027187603973\n",
      "Epoch:  67 Val Accuracy:  0.8434903047091413\n",
      "Epoch:  68 Training Accuracy:  0.8630638338324065\n",
      "Epoch:  68 Val Accuracy:  0.8494921514312096\n",
      "Epoch:  69 Training Accuracy:  0.8657730880745282\n",
      "Epoch:  69 Val Accuracy:  0.84533702677747\n",
      "Epoch:  70 Training Accuracy:  0.8656304957459955\n",
      "Epoch:  70 Val Accuracy:  0.8462603878116344\n",
      "Epoch:  71 Training Accuracy:  0.8653928418651077\n",
      "Epoch:  71 Val Accuracy:  0.8457987072945522\n",
      "Epoch:  72 Training Accuracy:  0.8639669185797804\n",
      "Epoch:  72 Val Accuracy:  0.8434903047091413\n",
      "Epoch:  73 Training Accuracy:  0.8636342031465374\n",
      "Epoch:  73 Val Accuracy:  0.8481071098799631\n",
      "Epoch:  74 Training Accuracy:  0.8639669185797804\n",
      "Epoch:  74 Val Accuracy:  0.84533702677747\n",
      "Epoch:  75 Training Accuracy:  0.8651076572080422\n",
      "Epoch:  75 Val Accuracy:  0.8471837488457987\n",
      "Epoch:  76 Training Accuracy:  0.8647274109986216\n",
      "Epoch:  76 Val Accuracy:  0.8462603878116344\n",
      "Epoch:  77 Training Accuracy:  0.866248395836304\n",
      "Epoch:  77 Val Accuracy:  0.8467220683287165\n",
      "Epoch:  78 Training Accuracy:  0.8668187651504349\n",
      "Epoch:  78 Val Accuracy:  0.8416435826408125\n",
      "Epoch:  79 Training Accuracy:  0.8654879034174628\n",
      "Epoch:  79 Val Accuracy:  0.8457987072945522\n",
      "Epoch:  80 Training Accuracy:  0.8663434573886591\n",
      "Epoch:  80 Val Accuracy:  0.8471837488457987\n",
      "Epoch:  81 Training Accuracy:  0.8660582727315937\n",
      "Epoch:  81 Val Accuracy:  0.8471837488457987\n",
      "Epoch:  82 Training Accuracy:  0.8665335804933695\n",
      "Epoch:  82 Val Accuracy:  0.8481071098799631\n",
      "Epoch:  83 Training Accuracy:  0.8666761728219022\n",
      "Epoch:  83 Val Accuracy:  0.8467220683287165\n",
      "Epoch:  84 Training Accuracy:  0.8667712343742573\n",
      "Epoch:  84 Val Accuracy:  0.8462603878116344\n",
      "Epoch:  85 Training Accuracy:  0.8668662959266125\n",
      "Epoch:  85 Val Accuracy:  0.8462603878116344\n",
      "Epoch:  86 Training Accuracy:  0.8666286420457247\n",
      "Epoch:  86 Val Accuracy:  0.8471837488457987\n",
      "Epoch:  87 Training Accuracy:  0.8671990113598556\n",
      "Epoch:  87 Val Accuracy:  0.8490304709141274\n",
      "Epoch:  88 Training Accuracy:  0.8671039498075004\n",
      "Epoch:  88 Val Accuracy:  0.84533702677747\n",
      "Epoch:  89 Training Accuracy:  0.8673416036883882\n",
      "Epoch:  89 Val Accuracy:  0.8430286241920592\n",
      "Epoch:  90 Training Accuracy:  0.8663909881648367\n",
      "Epoch:  90 Val Accuracy:  0.8444136657433057\n",
      "Epoch:  91 Training Accuracy:  0.8666286420457247\n",
      "Epoch:  91 Val Accuracy:  0.8471837488457987\n",
      "Epoch:  92 Training Accuracy:  0.8664385189410143\n",
      "Epoch:  92 Val Accuracy:  0.84533702677747\n",
      "Epoch:  93 Training Accuracy:  0.866248395836304\n",
      "Epoch:  93 Val Accuracy:  0.8476454293628809\n",
      "Epoch:  94 Training Accuracy:  0.8667712343742573\n",
      "Epoch:  94 Val Accuracy:  0.8444136657433057\n",
      "Epoch:  95 Training Accuracy:  0.8661533342839488\n",
      "Epoch:  95 Val Accuracy:  0.8467220683287165\n",
      "Epoch:  96 Training Accuracy:  0.8662008650601264\n",
      "Epoch:  96 Val Accuracy:  0.8462603878116344\n",
      "Epoch:  97 Training Accuracy:  0.8663434573886591\n",
      "Epoch:  97 Val Accuracy:  0.8471837488457987\n",
      "Epoch:  98 Training Accuracy:  0.8673416036883882\n",
      "Epoch:  98 Val Accuracy:  0.8457987072945522\n",
      "Epoch:  99 Training Accuracy:  0.8657255572983507\n",
      "Epoch:  99 Val Accuracy:  0.8457987072945522\n",
      "Epoch:  0 Training Accuracy:  0.38381101763391795\n",
      "Epoch:  0 Val Accuracy:  0.40766389658356417\n",
      "Epoch:  1 Training Accuracy:  0.4361899329816056\n",
      "Epoch:  1 Val Accuracy:  0.44829178208679593\n",
      "Epoch:  2 Training Accuracy:  0.3326679024668473\n",
      "Epoch:  2 Val Accuracy:  0.3845798707294552\n",
      "Epoch:  3 Training Accuracy:  0.3736394315319169\n",
      "Epoch:  3 Val Accuracy:  0.37950138504155123\n",
      "Epoch:  4 Training Accuracy:  0.3710252388421503\n",
      "Epoch:  4 Val Accuracy:  0.3896583564173592\n",
      "Epoch:  5 Training Accuracy:  0.4319596939018014\n",
      "Epoch:  5 Val Accuracy:  0.3896583564173592\n",
      "Epoch:  6 Training Accuracy:  0.49793241123627546\n",
      "Epoch:  6 Val Accuracy:  0.45013850415512463\n",
      "Epoch:  7 Training Accuracy:  0.5138076904795855\n",
      "Epoch:  7 Val Accuracy:  0.487072945521699\n",
      "Epoch:  8 Training Accuracy:  0.5751223917486572\n",
      "Epoch:  8 Val Accuracy:  0.5590951061865189\n",
      "Epoch:  9 Training Accuracy:  0.6340130234326726\n",
      "Epoch:  9 Val Accuracy:  0.6126500461680517\n",
      "Epoch:  10 Training Accuracy:  0.6245068681971576\n",
      "Epoch:  10 Val Accuracy:  0.628808864265928\n",
      "Epoch:  11 Training Accuracy:  0.6707543134179381\n",
      "Epoch:  11 Val Accuracy:  0.6555863342566943\n",
      "Epoch:  12 Training Accuracy:  0.7011264793954085\n",
      "Epoch:  12 Val Accuracy:  0.7156048014773777\n",
      "Epoch:  13 Training Accuracy:  0.739436284994534\n",
      "Epoch:  13 Val Accuracy:  0.7599261311172668\n",
      "Epoch:  14 Training Accuracy:  0.770521412614668\n",
      "Epoch:  14 Val Accuracy:  0.7848568790397045\n",
      "Epoch:  15 Training Accuracy:  0.7785065830125006\n",
      "Epoch:  15 Val Accuracy:  0.7820867959372114\n",
      "Epoch:  16 Training Accuracy:  0.8085460335567279\n",
      "Epoch:  16 Val Accuracy:  0.8134810710987996\n",
      "Epoch:  17 Training Accuracy:  0.8063120870763819\n",
      "Epoch:  17 Val Accuracy:  0.8337950138504155\n",
      "Epoch:  18 Training Accuracy:  0.8018441941156899\n",
      "Epoch:  18 Val Accuracy:  0.832409972299169\n",
      "Epoch:  19 Training Accuracy:  0.8227102048576453\n",
      "Epoch:  19 Val Accuracy:  0.8084025854108957\n",
      "Epoch:  20 Training Accuracy:  0.8314558676743191\n",
      "Epoch:  20 Val Accuracy:  0.8231763619575254\n",
      "Epoch:  21 Training Accuracy:  0.8389181995341984\n",
      "Epoch:  21 Val Accuracy:  0.8245614035087719\n",
      "Epoch:  22 Training Accuracy:  0.834260183468796\n",
      "Epoch:  22 Val Accuracy:  0.8393351800554016\n",
      "Epoch:  23 Training Accuracy:  0.8379675840106469\n",
      "Epoch:  23 Val Accuracy:  0.8434903047091413\n",
      "Epoch:  24 Training Accuracy:  0.8349731451114597\n",
      "Epoch:  24 Val Accuracy:  0.830101569713758\n",
      "Epoch:  25 Training Accuracy:  0.8400114073862827\n",
      "Epoch:  25 Val Accuracy:  0.8356417359187442\n",
      "Epoch:  26 Training Accuracy:  0.8437663387043111\n",
      "Epoch:  26 Val Accuracy:  0.8351800554016621\n",
      "Epoch:  27 Training Accuracy:  0.851371262892723\n",
      "Epoch:  27 Val Accuracy:  0.8134810710987996\n",
      "Epoch:  28 Training Accuracy:  0.8459527544084795\n",
      "Epoch:  28 Val Accuracy:  0.8374884579870729\n",
      "Epoch:  29 Training Accuracy:  0.8449546081087504\n",
      "Epoch:  29 Val Accuracy:  0.8379501385041551\n",
      "Epoch:  30 Training Accuracy:  0.8539854555824896\n",
      "Epoch:  30 Val Accuracy:  0.8254847645429363\n",
      "Epoch:  31 Training Accuracy:  0.8546984172251533\n",
      "Epoch:  31 Val Accuracy:  0.8194829178208679\n",
      "Epoch:  32 Training Accuracy:  0.8567897713769665\n",
      "Epoch:  32 Val Accuracy:  0.8361034164358264\n",
      "Epoch:  33 Training Accuracy:  0.8568848329293217\n",
      "Epoch:  33 Val Accuracy:  0.8462603878116344\n",
      "Epoch:  34 Training Accuracy:  0.8541280479110224\n",
      "Epoch:  34 Val Accuracy:  0.8430286241920592\n",
      "Epoch:  35 Training Accuracy:  0.8605922334711725\n",
      "Epoch:  35 Val Accuracy:  0.8407202216066482\n",
      "Epoch:  36 Training Accuracy:  0.857407671467275\n",
      "Epoch:  36 Val Accuracy:  0.8370267774699908\n",
      "Epoch:  37 Training Accuracy:  0.8638718570274253\n",
      "Epoch:  37 Val Accuracy:  0.8379501385041551\n",
      "Epoch:  38 Training Accuracy:  0.8691953039593137\n",
      "Epoch:  38 Val Accuracy:  0.8393351800554016\n",
      "Epoch:  39 Training Accuracy:  0.8631588953847616\n",
      "Epoch:  39 Val Accuracy:  0.8351800554016621\n",
      "Epoch:  40 Training Accuracy:  0.8726175198440991\n",
      "Epoch:  40 Val Accuracy:  0.8393351800554016\n",
      "Epoch:  41 Training Accuracy:  0.8757070202956414\n",
      "Epoch:  41 Val Accuracy:  0.8439519852262235\n",
      "Epoch:  42 Training Accuracy:  0.8775607205665669\n",
      "Epoch:  42 Val Accuracy:  0.8471837488457987\n",
      "Epoch:  43 Training Accuracy:  0.8758971434003517\n",
      "Epoch:  43 Val Accuracy:  0.8421052631578947\n",
      "Epoch:  44 Training Accuracy:  0.8777508436712772\n",
      "Epoch:  44 Val Accuracy:  0.8416435826408125\n",
      "Epoch:  45 Training Accuracy:  0.8796045439422026\n",
      "Epoch:  45 Val Accuracy:  0.8434903047091413\n",
      "Epoch:  46 Training Accuracy:  0.8797471362707353\n",
      "Epoch:  46 Val Accuracy:  0.8393351800554016\n",
      "Epoch:  47 Training Accuracy:  0.8846428062170255\n",
      "Epoch:  47 Val Accuracy:  0.8388734995383195\n",
      "Epoch:  48 Training Accuracy:  0.8795094823898474\n",
      "Epoch:  48 Val Accuracy:  0.8439519852262235\n",
      "Epoch:  49 Training Accuracy:  0.8806502210181092\n",
      "Epoch:  49 Val Accuracy:  0.8393351800554016\n",
      "Epoch:  50 Training Accuracy:  0.884024906126717\n",
      "Epoch:  50 Val Accuracy:  0.8333333333333334\n",
      "Epoch:  51 Training Accuracy:  0.8844051523361377\n",
      "Epoch:  51 Val Accuracy:  0.8476454293628809\n",
      "Epoch:  52 Training Accuracy:  0.8875897143400352\n",
      "Epoch:  52 Val Accuracy:  0.84533702677747\n",
      "Epoch:  53 Training Accuracy:  0.8883026759826989\n",
      "Epoch:  53 Val Accuracy:  0.8448753462603878\n",
      "Epoch:  54 Training Accuracy:  0.8847378677693807\n",
      "Epoch:  54 Val Accuracy:  0.8342566943674977\n",
      "Epoch:  55 Training Accuracy:  0.8853557678596892\n",
      "Epoch:  55 Val Accuracy:  0.8365650969529086\n",
      "Epoch:  56 Training Accuracy:  0.884927990874091\n",
      "Epoch:  56 Val Accuracy:  0.8444136657433057\n",
      "Epoch:  57 Training Accuracy:  0.8826465136175674\n",
      "Epoch:  57 Val Accuracy:  0.8467220683287165\n",
      "Epoch:  58 Training Accuracy:  0.8896810684918485\n",
      "Epoch:  58 Val Accuracy:  0.8421052631578947\n",
      "Epoch:  59 Training Accuracy:  0.8893483530586055\n",
      "Epoch:  59 Val Accuracy:  0.850415512465374\n",
      "Epoch:  60 Training Accuracy:  0.8921526688530824\n",
      "Epoch:  60 Val Accuracy:  0.8448753462603878\n",
      "Epoch:  61 Training Accuracy:  0.8920100765245497\n",
      "Epoch:  61 Val Accuracy:  0.8411819021237303\n",
      "Epoch:  62 Training Accuracy:  0.8934835305860545\n",
      "Epoch:  62 Val Accuracy:  0.8407202216066482\n",
      "Epoch:  63 Training Accuracy:  0.8934359998098769\n",
      "Epoch:  63 Val Accuracy:  0.8439519852262235\n",
      "Epoch:  64 Training Accuracy:  0.8939113075716526\n",
      "Epoch:  64 Val Accuracy:  0.840258541089566\n",
      "Epoch:  65 Training Accuracy:  0.8953372308569799\n",
      "Epoch:  65 Val Accuracy:  0.8393351800554016\n",
      "Epoch:  66 Training Accuracy:  0.8924853842863254\n",
      "Epoch:  66 Val Accuracy:  0.8425669436749769\n",
      "Epoch:  67 Training Accuracy:  0.8941964922287181\n",
      "Epoch:  67 Val Accuracy:  0.8379501385041551\n",
      "Epoch:  68 Training Accuracy:  0.8948143923190266\n",
      "Epoch:  68 Val Accuracy:  0.8379501385041551\n",
      "Epoch:  69 Training Accuracy:  0.8953847616331575\n",
      "Epoch:  69 Val Accuracy:  0.8434903047091413\n",
      "Epoch:  70 Training Accuracy:  0.8951471077522696\n",
      "Epoch:  70 Val Accuracy:  0.8407202216066482\n",
      "Epoch:  71 Training Accuracy:  0.8943866153334284\n",
      "Epoch:  71 Val Accuracy:  0.8430286241920592\n",
      "Epoch:  72 Training Accuracy:  0.8934835305860545\n",
      "Epoch:  72 Val Accuracy:  0.8411819021237303\n",
      "Epoch:  73 Training Accuracy:  0.8948619230952042\n",
      "Epoch:  73 Val Accuracy:  0.8407202216066482\n",
      "Epoch:  74 Training Accuracy:  0.8969057464708399\n",
      "Epoch:  74 Val Accuracy:  0.8374884579870729\n",
      "Epoch:  75 Training Accuracy:  0.896668092589952\n",
      "Epoch:  75 Val Accuracy:  0.8388734995383195\n",
      "Epoch:  76 Training Accuracy:  0.8972859926802604\n",
      "Epoch:  76 Val Accuracy:  0.8379501385041551\n",
      "Epoch:  77 Training Accuracy:  0.8968106849184847\n",
      "Epoch:  77 Val Accuracy:  0.8384118190212373\n",
      "Epoch:  78 Training Accuracy:  0.8965730310375969\n",
      "Epoch:  78 Val Accuracy:  0.8379501385041551\n",
      "Epoch:  79 Training Accuracy:  0.8980464850991017\n",
      "Epoch:  79 Val Accuracy:  0.8374884579870729\n",
      "Epoch:  80 Training Accuracy:  0.8990921621750083\n",
      "Epoch:  80 Val Accuracy:  0.8351800554016621\n",
      "Epoch:  81 Training Accuracy:  0.8992347545035411\n",
      "Epoch:  81 Val Accuracy:  0.8365650969529086\n",
      "Epoch:  82 Training Accuracy:  0.8984742620846998\n",
      "Epoch:  82 Val Accuracy:  0.8384118190212373\n",
      "Epoch:  83 Training Accuracy:  0.8978088312182138\n",
      "Epoch:  83 Val Accuracy:  0.8379501385041551\n",
      "Epoch:  84 Training Accuracy:  0.8987594467417653\n",
      "Epoch:  84 Val Accuracy:  0.8356417359187442\n",
      "Epoch:  85 Training Accuracy:  0.8996150007129616\n",
      "Epoch:  85 Val Accuracy:  0.8397968605724838\n",
      "Epoch:  86 Training Accuracy:  0.8990446313988307\n",
      "Epoch:  86 Val Accuracy:  0.8384118190212373\n",
      "Epoch:  87 Training Accuracy:  0.8989971006226531\n",
      "Epoch:  87 Val Accuracy:  0.8397968605724838\n",
      "Epoch:  88 Training Accuracy:  0.8993773468320738\n",
      "Epoch:  88 Val Accuracy:  0.8397968605724838\n",
      "Epoch:  89 Training Accuracy:  0.8988069775179429\n",
      "Epoch:  89 Val Accuracy:  0.8379501385041551\n",
      "Epoch:  90 Training Accuracy:  0.8984267313085222\n",
      "Epoch:  90 Val Accuracy:  0.8393351800554016\n",
      "Epoch:  91 Training Accuracy:  0.8993773468320738\n",
      "Epoch:  91 Val Accuracy:  0.8384118190212373\n",
      "Epoch:  92 Training Accuracy:  0.8991396929511859\n",
      "Epoch:  92 Val Accuracy:  0.8384118190212373\n",
      "Epoch:  93 Training Accuracy:  0.8993773468320738\n",
      "Epoch:  93 Val Accuracy:  0.8379501385041551\n",
      "Epoch:  94 Training Accuracy:  0.8998526545938496\n",
      "Epoch:  94 Val Accuracy:  0.8374884579870729\n",
      "Epoch:  95 Training Accuracy:  0.8996625314891392\n",
      "Epoch:  95 Val Accuracy:  0.8370267774699908\n",
      "Epoch:  96 Training Accuracy:  0.8994724083844289\n",
      "Epoch:  96 Val Accuracy:  0.8374884579870729\n",
      "Epoch:  97 Training Accuracy:  0.8992822852797187\n",
      "Epoch:  97 Val Accuracy:  0.8365650969529086\n",
      "Epoch:  98 Training Accuracy:  0.9001853700270925\n",
      "Epoch:  98 Val Accuracy:  0.8361034164358264\n",
      "Epoch:  99 Training Accuracy:  0.9000427776985598\n",
      "Epoch:  99 Val Accuracy:  0.8370267774699908\n",
      "Epoch:  0 Training Accuracy:  0.3326679024668473\n",
      "Epoch:  0 Val Accuracy:  0.3845798707294552\n",
      "Epoch:  1 Training Accuracy:  0.3326679024668473\n",
      "Epoch:  1 Val Accuracy:  0.3845798707294552\n",
      "Epoch:  2 Training Accuracy:  0.3326679024668473\n",
      "Epoch:  2 Val Accuracy:  0.3845798707294552\n",
      "Epoch:  3 Training Accuracy:  0.33176481771947336\n",
      "Epoch:  3 Val Accuracy:  0.37119113573407203\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m y_pred\u001b[38;5;241m=\u001b[39mmodel(x)\n\u001b[1;32m     22\u001b[0m loss\u001b[38;5;241m=\u001b[39mloss_fn(y_pred,y)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m loss_sum\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_classes=4\n",
    "for dropout in [0.1]:\n",
    "    for layer in [3,2,4]:\n",
    "            for hidden_dim in [10,16,32,64]:\n",
    "                if layer==3 and hidden_dim!=64:\n",
    "                    continue\n",
    "                torch.manual_seed(0)\n",
    "                model=LSTMModel(10,hidden_dim,layer,output_classes,dropout)\n",
    "                optimizer=torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "                scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=20,gamma=0.3)\n",
    "                if not os.path.exists('model4_3s/0.01_'+str(dropout)+'_'+str(layer)+'_'+str(hidden_dim)):\n",
    "                    os.makedirs('model4_3s/0.01_'+str(dropout)+'_'+str(layer)+'_'+str(hidden_dim))\n",
    "                filename='model4_3s/0.01_'+str(dropout)+'_'+str(layer)+'_'+str(hidden_dim)\n",
    "                f=open(filename+'/results.txt','w')\n",
    "                loss_arr=[]\n",
    "                training_accuracy=[]\n",
    "                val_accuracy=[]\n",
    "                for epoch in range(100):\n",
    "                    model.train()\n",
    "                    loss_sum=0\n",
    "                    for i,(x,y) in enumerate(train):\n",
    "                        optimizer.zero_grad()\n",
    "                        y_pred=model(x)\n",
    "                        loss=loss_fn(y_pred,y)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        loss_sum+=loss.item()\n",
    "                    scheduler.step()\n",
    "                    loss_sum/=len(train)\n",
    "                    loss_arr.append(loss_sum)\n",
    "                    model.eval()\n",
    "                    correct=0\n",
    "                    total=0\n",
    "                    #Calculate accuracy per class \n",
    "                    accuracy_per_class=[0]*output_classes\n",
    "                    fp_per_class=[0]*output_classes\n",
    "                    fn_per_class=[0]*output_classes\n",
    "                    tp_per_class=[0]*output_classes\n",
    "                    tn_per_class=[0]*output_classes\n",
    "                    fp_rate_per_class=[0]*output_classes\n",
    "                    for i,(x,y) in enumerate(train):\n",
    "                        y_pred=model(x)\n",
    "                        _,predicted=torch.max(y_pred.data,1)\n",
    "                        total+=y.size(0)\n",
    "                        y=torch.max(y,1)[1]\n",
    "                        correct+=(predicted==y).sum().item()\n",
    "                        for i in range(len(y)):\n",
    "                            if y[i]==predicted[i]:\n",
    "                                tp_per_class[y[i]]+=1\n",
    "                                for j in range(output_classes):\n",
    "                                    if j!=y[i]:\n",
    "                                        tn_per_class[j]+=1\n",
    "                            else:\n",
    "                                fp_per_class[predicted[i]]+=1\n",
    "                                fn_per_class[y[i]]+=1\n",
    "                                for j in range(output_classes):\n",
    "                                    if j!=y[i] and j!=predicted[i]:\n",
    "                                        tn_per_class[j]+=1\n",
    "                    for i in range(output_classes):\n",
    "                        fp_rate_per_class[i]=fp_per_class[i]/(fp_per_class[i]+tn_per_class[i])\n",
    "                        accuracy_per_class[i]=tp_per_class[i]/(tp_per_class[i]+fn_per_class[i])\n",
    "                    print('Epoch: ',epoch,'Training Accuracy: ',correct/total)\n",
    "                    training_accuracy.append(correct/total)\n",
    "                    f.write('Epoch: '+str(epoch)+' Training Accuracy: '+str(correct/total)+'\\n')\n",
    "                    f.write('Accuracy per class: '+str(accuracy_per_class)+'\\n')\n",
    "                    f.write('False positive rate per class: '+str(fp_rate_per_class)+'\\n')\n",
    "                    f.write('False positive per class: '+str(fp_per_class)+'\\n')\n",
    "                    f.write('False negative per class: '+str(fn_per_class)+'\\n')\n",
    "                    f.write('True positive per class: '+str(tp_per_class)+'\\n')\n",
    "                    f.write('True negative per class: '+str(tn_per_class)+'\\n')\n",
    "                    f.write('\\n')\n",
    "                    correct=0\n",
    "                    total=0\n",
    "                    accuracy_per_class=[0]*output_classes\n",
    "                    fp_per_class=[0]*output_classes\n",
    "                    fn_per_class=[0]*output_classes\n",
    "                    tp_per_class=[0]*output_classes\n",
    "                    tn_per_class=[0]*output_classes\n",
    "                    fp_rate_per_class=[0]*output_classes\n",
    "                    for i,(x,y) in enumerate(val):\n",
    "                        y_pred=model(x)\n",
    "                        _,predicted=torch.max(y_pred.data,1)\n",
    "                        total+=y.size(0)\n",
    "                        y=torch.max(y,1)[1]\n",
    "                        correct+=(predicted==y).sum().item()\n",
    "                        for i in range(len(y)):\n",
    "                            if y[i]==predicted[i]:\n",
    "                                tp_per_class[y[i]]+=1\n",
    "                                for j in range(output_classes):\n",
    "                                    if j!=y[i]:\n",
    "                                        tn_per_class[j]+=1\n",
    "                            else:\n",
    "                                fp_per_class[predicted[i]]+=1\n",
    "                                fn_per_class[y[i]]+=1\n",
    "                                for j in range(output_classes):\n",
    "                                    if j!=y[i] and j!=predicted[i]:\n",
    "                                        tn_per_class[j]+=1\n",
    "                    for i in range(output_classes):\n",
    "                        fp_rate_per_class[i]=fp_per_class[i]/(fp_per_class[i]+tn_per_class[i])\n",
    "                        if tp_per_class[i]+fn_per_class[i]==0:\n",
    "                            accuracy_per_class[i]=1\n",
    "                        else:\n",
    "                            accuracy_per_class[i]=tp_per_class[i]/(tp_per_class[i]+fn_per_class[i])\n",
    "                    print('Epoch: ',epoch,'Val Accuracy: ',correct/total)\n",
    "                    val_accuracy.append(correct/total)\n",
    "                    f.write('Epoch: '+str(epoch)+' Val Accuracy: '+str(correct/total)+'\\n')\n",
    "                    f.write('Accuracy per class: '+str(accuracy_per_class)+'\\n')\n",
    "                    f.write('False positive rate per class: '+str(fp_rate_per_class)+'\\n')\n",
    "                    f.write('False positive per class: '+str(fp_per_class)+'\\n')\n",
    "                    f.write('False negative per class: '+str(fn_per_class)+'\\n')\n",
    "                    f.write('True positive per class: '+str(tp_per_class)+'\\n')\n",
    "                    f.write('True negative per class: '+str(tn_per_class)+'\\n')\n",
    "                    f.write('\\n')\n",
    "                    torch.save(model,filename+'/model'+str(epoch)+'.pt')   \n",
    "                plt.plot(loss_arr)\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.savefig(filename+'/loss.png')\n",
    "                plt.close()\n",
    "                plt.plot(training_accuracy)\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Training Accuracy')\n",
    "                plt.savefig(filename+'/training_accuracy.png')\n",
    "                plt.close()\n",
    "                plt.plot(val_accuracy)\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Validation Accuracy')\n",
    "                plt.savefig(filename+'/val_accuracy.png')\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=torch.load('train6_3_dataloader.pt')\n",
    "val=torch.load('val6_3_dataloader.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Training Accuracy:  0.2298332582244254\n",
      "Epoch:  0 Val Accuracy:  0.26177285318559557\n",
      "Epoch:  1 Training Accuracy:  0.2568724650743578\n",
      "Epoch:  1 Val Accuracy:  0.3554939981532779\n",
      "Epoch:  2 Training Accuracy:  0.26528466276100343\n",
      "Epoch:  2 Val Accuracy:  0.33933518005540164\n",
      "Epoch:  3 Training Accuracy:  0.2540934354814481\n",
      "Epoch:  3 Val Accuracy:  0.3125577100646353\n",
      "Epoch:  4 Training Accuracy:  0.25259125732311855\n",
      "Epoch:  4 Val Accuracy:  0.2968605724838412\n",
      "Epoch:  5 Training Accuracy:  0.2552200691001953\n",
      "Epoch:  5 Val Accuracy:  0.3656509695290859\n",
      "Epoch:  6 Training Accuracy:  0.249061138651044\n",
      "Epoch:  6 Val Accuracy:  0.36934441366574333\n",
      "Epoch:  7 Training Accuracy:  0.2857893946222022\n",
      "Epoch:  7 Val Accuracy:  0.4039704524469067\n",
      "Epoch:  8 Training Accuracy:  0.2857142857142857\n",
      "Epoch:  8 Val Accuracy:  0.3333333333333333\n",
      "Epoch:  9 Training Accuracy:  0.259726603575184\n",
      "Epoch:  9 Val Accuracy:  0.38227146814404434\n",
      "Epoch:  10 Training Accuracy:  0.2779780681988884\n",
      "Epoch:  10 Val Accuracy:  0.36149584487534625\n",
      "Epoch:  11 Training Accuracy:  0.2972810575334235\n",
      "Epoch:  11 Val Accuracy:  0.3739612188365651\n",
      "Epoch:  12 Training Accuracy:  0.30974913624755895\n",
      "Epoch:  12 Val Accuracy:  0.3864265927977839\n",
      "Epoch:  13 Training Accuracy:  0.3181613339342046\n",
      "Epoch:  13 Val Accuracy:  0.36334256694367495\n",
      "Epoch:  14 Training Accuracy:  0.32724951179209855\n",
      "Epoch:  14 Val Accuracy:  0.34856879039704525\n",
      "Epoch:  15 Training Accuracy:  0.334384858044164\n",
      "Epoch:  15 Val Accuracy:  0.4168975069252078\n",
      "Epoch:  16 Training Accuracy:  0.34474988733663814\n",
      "Epoch:  16 Val Accuracy:  0.41089566020313945\n",
      "Epoch:  17 Training Accuracy:  0.3438485804416404\n",
      "Epoch:  17 Val Accuracy:  0.40627885503231764\n",
      "Epoch:  18 Training Accuracy:  0.35458915427369686\n",
      "Epoch:  18 Val Accuracy:  0.38550323176361956\n",
      "Epoch:  19 Training Accuracy:  0.3640528766711732\n",
      "Epoch:  19 Val Accuracy:  0.43998153277931673\n",
      "Epoch:  20 Training Accuracy:  0.3711131140153222\n",
      "Epoch:  20 Val Accuracy:  0.44598337950138506\n",
      "Epoch:  21 Training Accuracy:  0.3836563016373742\n",
      "Epoch:  21 Val Accuracy:  0.42197599261311175\n",
      "Epoch:  22 Training Accuracy:  0.3881628361123629\n",
      "Epoch:  22 Val Accuracy:  0.43674976915974145\n",
      "Epoch:  23 Training Accuracy:  0.3924440438636022\n",
      "Epoch:  23 Val Accuracy:  0.4372114496768236\n",
      "Epoch:  24 Training Accuracy:  0.4025086375244104\n",
      "Epoch:  24 Val Accuracy:  0.43998153277931673\n",
      "Epoch:  25 Training Accuracy:  0.40386059786690703\n",
      "Epoch:  25 Val Accuracy:  0.46121883656509693\n",
      "Epoch:  26 Training Accuracy:  0.418281508186871\n",
      "Epoch:  26 Val Accuracy:  0.4584487534626039\n",
      "Epoch:  27 Training Accuracy:  0.42301336938560913\n",
      "Epoch:  27 Val Accuracy:  0.42105263157894735\n",
      "Epoch:  28 Training Accuracy:  0.4280456662160132\n",
      "Epoch:  28 Val Accuracy:  0.46075715604801476\n",
      "Epoch:  29 Training Accuracy:  0.42541685443893645\n",
      "Epoch:  29 Val Accuracy:  0.47045244690674054\n",
      "Epoch:  30 Training Accuracy:  0.43120024034850535\n",
      "Epoch:  30 Val Accuracy:  0.481994459833795\n",
      "Epoch:  31 Training Accuracy:  0.4429923388913925\n",
      "Epoch:  31 Val Accuracy:  0.46860572483841184\n",
      "Epoch:  32 Training Accuracy:  0.43758449752140605\n",
      "Epoch:  32 Val Accuracy:  0.47876269621421974\n",
      "Epoch:  33 Training Accuracy:  0.43067447799309\n",
      "Epoch:  33 Val Accuracy:  0.48522622345337024\n",
      "Epoch:  34 Training Accuracy:  0.4457713684843022\n",
      "Epoch:  34 Val Accuracy:  0.4579870729455217\n",
      "Epoch:  35 Training Accuracy:  0.45290671473636773\n",
      "Epoch:  35 Val Accuracy:  0.4644506001846722\n",
      "Epoch:  36 Training Accuracy:  0.45320715036803366\n",
      "Epoch:  36 Val Accuracy:  0.481994459833795\n",
      "Epoch:  37 Training Accuracy:  0.4522307345651194\n",
      "Epoch:  37 Val Accuracy:  0.49261311172668515\n",
      "Epoch:  38 Training Accuracy:  0.45080366531470634\n",
      "Epoch:  38 Val Accuracy:  0.48614958448753465\n",
      "Epoch:  39 Training Accuracy:  0.45778879375093884\n",
      "Epoch:  39 Val Accuracy:  0.4935364727608495\n",
      "Epoch:  40 Training Accuracy:  0.460793150067598\n",
      "Epoch:  40 Val Accuracy:  0.4995383194829178\n",
      "Epoch:  41 Training Accuracy:  0.46274598167342645\n",
      "Epoch:  41 Val Accuracy:  0.5032317636195752\n",
      "Epoch:  42 Training Accuracy:  0.46319663512092535\n",
      "Epoch:  42 Val Accuracy:  0.49907663896583565\n",
      "Epoch:  43 Training Accuracy:  0.4588403184617696\n",
      "Epoch:  43 Val Accuracy:  0.4976915974145891\n",
      "Epoch:  44 Training Accuracy:  0.46282109058134296\n",
      "Epoch:  44 Val Accuracy:  0.5\n",
      "Epoch:  45 Training Accuracy:  0.4604927144359321\n",
      "Epoch:  45 Val Accuracy:  0.510156971375808\n",
      "Epoch:  46 Training Accuracy:  0.4650743578188373\n",
      "Epoch:  46 Val Accuracy:  0.5064635272391506\n",
      "Epoch:  47 Training Accuracy:  0.4636472885684242\n",
      "Epoch:  47 Val Accuracy:  0.5069252077562327\n",
      "Epoch:  48 Training Accuracy:  0.4651494667267538\n",
      "Epoch:  48 Val Accuracy:  0.515697137580794\n",
      "Epoch:  49 Training Accuracy:  0.47378699113714884\n",
      "Epoch:  49 Val Accuracy:  0.5138504155124654\n",
      "Epoch:  50 Training Accuracy:  0.47175905062340395\n",
      "Epoch:  50 Val Accuracy:  0.5133887349953832\n",
      "Epoch:  51 Training Accuracy:  0.4747634069400631\n",
      "Epoch:  51 Val Accuracy:  0.5120036934441367\n",
      "Epoch:  52 Training Accuracy:  0.46852936758299535\n",
      "Epoch:  52 Val Accuracy:  0.5138504155124654\n",
      "Epoch:  53 Training Accuracy:  0.4758149316508938\n",
      "Epoch:  53 Val Accuracy:  0.525854108956602\n",
      "Epoch:  54 Training Accuracy:  0.4722097040709028\n",
      "Epoch:  54 Val Accuracy:  0.5166204986149584\n",
      "Epoch:  55 Training Accuracy:  0.4772420009013069\n",
      "Epoch:  55 Val Accuracy:  0.523084025854109\n",
      "Epoch:  56 Training Accuracy:  0.4775424365329728\n",
      "Epoch:  56 Val Accuracy:  0.525854108956602\n",
      "Epoch:  57 Training Accuracy:  0.4784437434279706\n",
      "Epoch:  57 Val Accuracy:  0.5249307479224377\n",
      "Epoch:  58 Training Accuracy:  0.4683791497671624\n",
      "Epoch:  58 Val Accuracy:  0.5290858725761773\n",
      "Epoch:  59 Training Accuracy:  0.47709178308547395\n",
      "Epoch:  59 Val Accuracy:  0.530932594644506\n",
      "Epoch:  60 Training Accuracy:  0.4807721195733814\n",
      "Epoch:  60 Val Accuracy:  0.523084025854109\n",
      "Epoch:  61 Training Accuracy:  0.47957037704671773\n",
      "Epoch:  61 Val Accuracy:  0.5240073868882733\n",
      "Epoch:  62 Training Accuracy:  0.4808472284812979\n",
      "Epoch:  62 Val Accuracy:  0.5249307479224377\n",
      "Epoch:  63 Training Accuracy:  0.47866907015172\n",
      "Epoch:  63 Val Accuracy:  0.5281625115420129\n",
      "Epoch:  64 Training Accuracy:  0.48137299083671325\n",
      "Epoch:  64 Val Accuracy:  0.5244690674053555\n",
      "Epoch:  65 Training Accuracy:  0.4746882980321466\n",
      "Epoch:  65 Val Accuracy:  0.5272391505078485\n",
      "Epoch:  66 Training Accuracy:  0.4815983175604627\n",
      "Epoch:  66 Val Accuracy:  0.5244690674053555\n",
      "Epoch:  67 Training Accuracy:  0.48347604025837465\n",
      "Epoch:  67 Val Accuracy:  0.5290858725761773\n",
      "Epoch:  68 Training Accuracy:  0.480546792849632\n",
      "Epoch:  68 Val Accuracy:  0.5267774699907664\n",
      "Epoch:  69 Training Accuracy:  0.48407691152170645\n",
      "Epoch:  69 Val Accuracy:  0.5281625115420129\n",
      "Epoch:  70 Training Accuracy:  0.4782184167042211\n",
      "Epoch:  70 Val Accuracy:  0.5364727608494921\n",
      "Epoch:  71 Training Accuracy:  0.47964548595463424\n",
      "Epoch:  71 Val Accuracy:  0.530932594644506\n",
      "Epoch:  72 Training Accuracy:  0.48167342646837913\n",
      "Epoch:  72 Val Accuracy:  0.5332409972299169\n",
      "Epoch:  73 Training Accuracy:  0.48234940663962744\n",
      "Epoch:  73 Val Accuracy:  0.5327793167128347\n",
      "Epoch:  74 Training Accuracy:  0.480396575033799\n",
      "Epoch:  74 Val Accuracy:  0.5346260387811634\n",
      "Epoch:  75 Training Accuracy:  0.4779930899804717\n",
      "Epoch:  75 Val Accuracy:  0.5424746075715605\n",
      "Epoch:  76 Training Accuracy:  0.47919483250713535\n",
      "Epoch:  76 Val Accuracy:  0.538781163434903\n",
      "Epoch:  77 Training Accuracy:  0.4810725552050473\n",
      "Epoch:  77 Val Accuracy:  0.5397045244690674\n",
      "Epoch:  78 Training Accuracy:  0.4850533273246207\n",
      "Epoch:  78 Val Accuracy:  0.5332409972299169\n",
      "Epoch:  79 Training Accuracy:  0.4875319212858645\n",
      "Epoch:  79 Val Accuracy:  0.5295475530932595\n",
      "Epoch:  80 Training Accuracy:  0.48385158479795704\n",
      "Epoch:  80 Val Accuracy:  0.5318559556786704\n",
      "Epoch:  81 Training Accuracy:  0.4831756046267087\n",
      "Epoch:  81 Val Accuracy:  0.5300092336103417\n",
      "Epoch:  82 Training Accuracy:  0.48452756496920535\n",
      "Epoch:  82 Val Accuracy:  0.5318559556786704\n",
      "Epoch:  83 Training Accuracy:  0.4837764758900406\n",
      "Epoch:  83 Val Accuracy:  0.5318559556786704\n",
      "Epoch:  84 Training Accuracy:  0.48302538681087576\n",
      "Epoch:  84 Val Accuracy:  0.5304709141274239\n",
      "Epoch:  85 Training Accuracy:  0.4843773471533724\n",
      "Epoch:  85 Val Accuracy:  0.5300092336103417\n",
      "Epoch:  86 Training Accuracy:  0.4844524560612889\n",
      "Epoch:  86 Val Accuracy:  0.5318559556786704\n",
      "Epoch:  87 Training Accuracy:  0.4846026738771218\n",
      "Epoch:  87 Val Accuracy:  0.5323176361957526\n",
      "Epoch:  88 Training Accuracy:  0.4846777827850383\n",
      "Epoch:  88 Val Accuracy:  0.5318559556786704\n",
      "Epoch:  89 Training Accuracy:  0.48497821841670424\n",
      "Epoch:  89 Val Accuracy:  0.5332409972299169\n",
      "Epoch:  90 Training Accuracy:  0.4843773471533724\n",
      "Epoch:  90 Val Accuracy:  0.5346260387811634\n",
      "Epoch:  91 Training Accuracy:  0.48310049571879227\n",
      "Epoch:  91 Val Accuracy:  0.5300092336103417\n",
      "Epoch:  92 Training Accuracy:  0.4843773471533724\n",
      "Epoch:  92 Val Accuracy:  0.5281625115420129\n",
      "Epoch:  93 Training Accuracy:  0.48430223824545593\n",
      "Epoch:  93 Val Accuracy:  0.530932594644506\n",
      "Epoch:  94 Training Accuracy:  0.4854288718642031\n",
      "Epoch:  94 Val Accuracy:  0.5332409972299169\n",
      "Epoch:  95 Training Accuracy:  0.4836262580742076\n",
      "Epoch:  95 Val Accuracy:  0.5323176361957526\n",
      "Epoch:  96 Training Accuracy:  0.48370136698212407\n",
      "Epoch:  96 Val Accuracy:  0.5327793167128347\n",
      "Epoch:  97 Training Accuracy:  0.48452756496920535\n",
      "Epoch:  97 Val Accuracy:  0.5318559556786704\n",
      "Epoch:  98 Training Accuracy:  0.4824996244554604\n",
      "Epoch:  98 Val Accuracy:  0.5341643582640813\n",
      "Epoch:  99 Training Accuracy:  0.48557908968003605\n",
      "Epoch:  99 Val Accuracy:  0.5323176361957526\n",
      "Epoch:  0 Training Accuracy:  0.22720444644734866\n",
      "Epoch:  0 Val Accuracy:  0.23130193905817176\n",
      "Epoch:  1 Training Accuracy:  0.2568724650743578\n",
      "Epoch:  1 Val Accuracy:  0.29270544783010155\n",
      "Epoch:  2 Training Accuracy:  0.2673126032747484\n",
      "Epoch:  2 Val Accuracy:  0.3471837488457987\n",
      "Epoch:  3 Training Accuracy:  0.2607781282860147\n",
      "Epoch:  3 Val Accuracy:  0.25715604801477376\n",
      "Epoch:  4 Training Accuracy:  0.27174402884182064\n",
      "Epoch:  4 Val Accuracy:  0.31763619575253926\n",
      "Epoch:  5 Training Accuracy:  0.297205948625507\n",
      "Epoch:  5 Val Accuracy:  0.3591874422899354\n",
      "Epoch:  6 Training Accuracy:  0.2999849782184167\n",
      "Epoch:  6 Val Accuracy:  0.4168975069252078\n",
      "Epoch:  7 Training Accuracy:  0.3205648189875319\n",
      "Epoch:  7 Val Accuracy:  0.37580794090489383\n",
      "Epoch:  8 Training Accuracy:  0.3226678684091933\n",
      "Epoch:  8 Val Accuracy:  0.40443213296398894\n",
      "Epoch:  9 Training Accuracy:  0.32236743277752744\n",
      "Epoch:  9 Val Accuracy:  0.2917820867959372\n",
      "Epoch:  10 Training Accuracy:  0.330929848280006\n",
      "Epoch:  10 Val Accuracy:  0.39935364727608497\n",
      "Epoch:  11 Training Accuracy:  0.33941715487456814\n",
      "Epoch:  11 Val Accuracy:  0.37488457987072943\n",
      "Epoch:  12 Training Accuracy:  0.34895598617996093\n",
      "Epoch:  12 Val Accuracy:  0.3716528162511542\n",
      "Epoch:  13 Training Accuracy:  0.3572179660507736\n",
      "Epoch:  13 Val Accuracy:  0.3545706371191136\n",
      "Epoch:  14 Training Accuracy:  0.3655550548295028\n",
      "Epoch:  14 Val Accuracy:  0.33518005540166207\n",
      "Epoch:  15 Training Accuracy:  0.3721646387261529\n",
      "Epoch:  15 Val Accuracy:  0.3864265927977839\n",
      "Epoch:  16 Training Accuracy:  0.37066246056782337\n",
      "Epoch:  16 Val Accuracy:  0.3397968605724838\n",
      "Epoch:  17 Training Accuracy:  0.3772720444644735\n",
      "Epoch:  17 Val Accuracy:  0.3721144967682364\n",
      "Epoch:  18 Training Accuracy:  0.38508337088778727\n",
      "Epoch:  18 Val Accuracy:  0.3411819021237304\n",
      "Epoch:  19 Training Accuracy:  0.3818536878473787\n",
      "Epoch:  19 Val Accuracy:  0.37442289935364725\n",
      "Epoch:  20 Training Accuracy:  0.4041610334985729\n",
      "Epoch:  20 Val Accuracy:  0.36657433056325023\n",
      "Epoch:  21 Training Accuracy:  0.40596364728856843\n",
      "Epoch:  21 Val Accuracy:  0.3688827331486611\n",
      "Epoch:  22 Training Accuracy:  0.4031846176956587\n",
      "Epoch:  22 Val Accuracy:  0.39289012003693446\n",
      "Epoch:  23 Training Accuracy:  0.40558810274898605\n",
      "Epoch:  23 Val Accuracy:  0.3896583564173592\n",
      "Epoch:  24 Training Accuracy:  0.41647889439687547\n",
      "Epoch:  24 Val Accuracy:  0.36149584487534625\n",
      "Epoch:  25 Training Accuracy:  0.4192579239897852\n",
      "Epoch:  25 Val Accuracy:  0.3698060941828255\n",
      "Epoch:  26 Training Accuracy:  0.41625356767312605\n",
      "Epoch:  26 Val Accuracy:  0.3527239150507849\n",
      "Epoch:  27 Training Accuracy:  0.4310500225326724\n",
      "Epoch:  27 Val Accuracy:  0.3573407202216066\n",
      "Epoch:  28 Training Accuracy:  0.421736517951029\n",
      "Epoch:  28 Val Accuracy:  0.358264081255771\n",
      "Epoch:  29 Training Accuracy:  0.43330328977016674\n",
      "Epoch:  29 Val Accuracy:  0.3670360110803324\n",
      "Epoch:  30 Training Accuracy:  0.4345050322968304\n",
      "Epoch:  30 Val Accuracy:  0.37442289935364725\n",
      "Epoch:  31 Training Accuracy:  0.44351810124680785\n",
      "Epoch:  31 Val Accuracy:  0.38134810710988\n",
      "Epoch:  32 Training Accuracy:  0.4499023584197086\n",
      "Epoch:  32 Val Accuracy:  0.4007386888273315\n",
      "Epoch:  33 Training Accuracy:  0.4532822592759501\n",
      "Epoch:  33 Val Accuracy:  0.39012003693444136\n",
      "Epoch:  34 Training Accuracy:  0.46267087276551\n",
      "Epoch:  34 Val Accuracy:  0.41043397968605727\n",
      "Epoch:  35 Training Accuracy:  0.46282109058134296\n",
      "Epoch:  35 Val Accuracy:  0.38227146814404434\n",
      "Epoch:  36 Training Accuracy:  0.4637223974763407\n",
      "Epoch:  36 Val Accuracy:  0.4067405355493998\n",
      "Epoch:  37 Training Accuracy:  0.4757398227429773\n",
      "Epoch:  37 Val Accuracy:  0.4159741458910434\n",
      "Epoch:  38 Training Accuracy:  0.4785939612438035\n",
      "Epoch:  38 Val Accuracy:  0.44321329639889195\n",
      "Epoch:  39 Training Accuracy:  0.48768213910169744\n",
      "Epoch:  39 Val Accuracy:  0.3873499538319483\n",
      "Epoch:  40 Training Accuracy:  0.5014270692504131\n",
      "Epoch:  40 Val Accuracy:  0.41874422899353647\n",
      "Epoch:  41 Training Accuracy:  0.5051074057383206\n",
      "Epoch:  41 Val Accuracy:  0.40812557710064634\n",
      "Epoch:  42 Training Accuracy:  0.5036803364879074\n",
      "Epoch:  42 Val Accuracy:  0.4344413665743306\n",
      "Epoch:  43 Training Accuracy:  0.5039056632116569\n",
      "Epoch:  43 Val Accuracy:  0.43951985226223456\n",
      "Epoch:  44 Training Accuracy:  0.5061589304491513\n",
      "Epoch:  44 Val Accuracy:  0.4261311172668513\n",
      "Epoch:  45 Training Accuracy:  0.508787742226228\n",
      "Epoch:  45 Val Accuracy:  0.42197599261311175\n",
      "Epoch:  46 Training Accuracy:  0.5114165540033048\n",
      "Epoch:  46 Val Accuracy:  0.42566943674976915\n",
      "Epoch:  47 Training Accuracy:  0.5157728706624606\n",
      "Epoch:  47 Val Accuracy:  0.4376731301939058\n",
      "Epoch:  48 Training Accuracy:  0.5145711281357969\n",
      "Epoch:  48 Val Accuracy:  0.4242843951985226\n",
      "Epoch:  49 Training Accuracy:  0.5137449301487156\n",
      "Epoch:  49 Val Accuracy:  0.4242843951985226\n",
      "Epoch:  50 Training Accuracy:  0.5157728706624606\n",
      "Epoch:  50 Val Accuracy:  0.4358264081255771\n",
      "Epoch:  51 Training Accuracy:  0.5195283160582845\n",
      "Epoch:  51 Val Accuracy:  0.43674976915974145\n",
      "Epoch:  52 Training Accuracy:  0.5220820189274448\n",
      "Epoch:  52 Val Accuracy:  0.43536472760849493\n",
      "Epoch:  53 Training Accuracy:  0.5235090881778579\n",
      "Epoch:  53 Val Accuracy:  0.4261311172668513\n",
      "Epoch:  54 Training Accuracy:  0.5268889890340994\n",
      "Epoch:  54 Val Accuracy:  0.4298245614035088\n",
      "Epoch:  55 Training Accuracy:  0.5286916028240949\n",
      "Epoch:  55 Val Accuracy:  0.4376731301939058\n",
      "Epoch:  56 Training Accuracy:  0.5244103950728557\n",
      "Epoch:  56 Val Accuracy:  0.4422899353647276\n",
      "Epoch:  57 Training Accuracy:  0.5295929097190927\n",
      "Epoch:  57 Val Accuracy:  0.438134810710988\n",
      "Epoch:  58 Training Accuracy:  0.5247108307045215\n",
      "Epoch:  58 Val Accuracy:  0.4427516158818098\n",
      "Epoch:  59 Training Accuracy:  0.5328225927595013\n",
      "Epoch:  59 Val Accuracy:  0.44321329639889195\n",
      "Epoch:  60 Training Accuracy:  0.535751840168244\n",
      "Epoch:  60 Val Accuracy:  0.4409048938134811\n",
      "Epoch:  61 Training Accuracy:  0.533573681838666\n",
      "Epoch:  61 Val Accuracy:  0.4422899353647276\n",
      "Epoch:  62 Training Accuracy:  0.5350758599969957\n",
      "Epoch:  62 Val Accuracy:  0.4390581717451524\n",
      "Epoch:  63 Training Accuracy:  0.5347003154574133\n",
      "Epoch:  63 Val Accuracy:  0.44182825484764543\n",
      "Epoch:  64 Training Accuracy:  0.5355265134444945\n",
      "Epoch:  64 Val Accuracy:  0.4372114496768236\n",
      "Epoch:  65 Training Accuracy:  0.5380051074057384\n",
      "Epoch:  65 Val Accuracy:  0.4441366574330563\n",
      "Epoch:  66 Training Accuracy:  0.5393570677482349\n",
      "Epoch:  66 Val Accuracy:  0.4455216989843029\n",
      "Epoch:  67 Training Accuracy:  0.5379299984978219\n",
      "Epoch:  67 Val Accuracy:  0.44321329639889195\n",
      "Epoch:  68 Training Accuracy:  0.5405588102748986\n",
      "Epoch:  68 Val Accuracy:  0.45013850415512463\n",
      "Epoch:  69 Training Accuracy:  0.5422112062490612\n",
      "Epoch:  69 Val Accuracy:  0.4422899353647276\n",
      "Epoch:  70 Training Accuracy:  0.541084572630314\n",
      "Epoch:  70 Val Accuracy:  0.4422899353647276\n",
      "Epoch:  71 Training Accuracy:  0.5392819588403185\n",
      "Epoch:  71 Val Accuracy:  0.4441366574330563\n",
      "Epoch:  72 Training Accuracy:  0.5422112062490612\n",
      "Epoch:  72 Val Accuracy:  0.4455216989843029\n",
      "Epoch:  73 Training Accuracy:  0.5411596815382305\n",
      "Epoch:  73 Val Accuracy:  0.4376731301939058\n",
      "Epoch:  74 Training Accuracy:  0.5424365329728106\n",
      "Epoch:  74 Val Accuracy:  0.4427516158818098\n",
      "Epoch:  75 Training Accuracy:  0.5425867507886435\n",
      "Epoch:  75 Val Accuracy:  0.44321329639889195\n",
      "Epoch:  76 Training Accuracy:  0.5410094637223974\n",
      "Epoch:  76 Val Accuracy:  0.44644506001846723\n",
      "Epoch:  77 Training Accuracy:  0.5408592459065645\n",
      "Epoch:  77 Val Accuracy:  0.4469067405355494\n",
      "Epoch:  78 Training Accuracy:  0.5432627309598919\n",
      "Epoch:  78 Val Accuracy:  0.4441366574330563\n",
      "Epoch:  79 Training Accuracy:  0.5435631665915578\n",
      "Epoch:  79 Val Accuracy:  0.4455216989843029\n",
      "Epoch:  80 Training Accuracy:  0.5420609884332281\n",
      "Epoch:  80 Val Accuracy:  0.44644506001846723\n",
      "Epoch:  81 Training Accuracy:  0.5439387111311401\n",
      "Epoch:  81 Val Accuracy:  0.44829178208679593\n",
      "Epoch:  82 Training Accuracy:  0.5448400180261379\n",
      "Epoch:  82 Val Accuracy:  0.44644506001846723\n",
      "Epoch:  83 Training Accuracy:  0.5439387111311401\n",
      "Epoch:  83 Val Accuracy:  0.4445983379501385\n",
      "Epoch:  84 Training Accuracy:  0.5434880576836413\n",
      "Epoch:  84 Val Accuracy:  0.44598337950138506\n",
      "Epoch:  85 Training Accuracy:  0.5434880576836413\n",
      "Epoch:  85 Val Accuracy:  0.4469067405355494\n",
      "Epoch:  86 Training Accuracy:  0.5446146913023885\n",
      "Epoch:  86 Val Accuracy:  0.44644506001846723\n",
      "Epoch:  87 Training Accuracy:  0.546267087276551\n",
      "Epoch:  87 Val Accuracy:  0.4450600184672207\n",
      "Epoch:  88 Training Accuracy:  0.5467928496319664\n",
      "Epoch:  88 Val Accuracy:  0.44182825484764543\n",
      "Epoch:  89 Training Accuracy:  0.5440138200390566\n",
      "Epoch:  89 Val Accuracy:  0.44182825484764543\n",
      "Epoch:  90 Training Accuracy:  0.5438636022232237\n",
      "Epoch:  90 Val Accuracy:  0.4450600184672207\n",
      "Epoch:  91 Training Accuracy:  0.5461919783686345\n",
      "Epoch:  91 Val Accuracy:  0.4469067405355494\n",
      "Epoch:  92 Training Accuracy:  0.5452906714736367\n",
      "Epoch:  92 Val Accuracy:  0.44967682363804246\n",
      "Epoch:  93 Training Accuracy:  0.5460417605528015\n",
      "Epoch:  93 Val Accuracy:  0.44644506001846723\n",
      "Epoch:  94 Training Accuracy:  0.5456662160132192\n",
      "Epoch:  94 Val Accuracy:  0.44321329639889195\n",
      "Epoch:  95 Training Accuracy:  0.5455911071053027\n",
      "Epoch:  95 Val Accuracy:  0.4445983379501385\n",
      "Epoch:  96 Training Accuracy:  0.5458164338290521\n",
      "Epoch:  96 Val Accuracy:  0.4441366574330563\n",
      "Epoch:  97 Training Accuracy:  0.5470932852636322\n",
      "Epoch:  97 Val Accuracy:  0.4450600184672207\n",
      "Epoch:  98 Training Accuracy:  0.5471683941715487\n",
      "Epoch:  98 Val Accuracy:  0.44598337950138506\n",
      "Epoch:  99 Training Accuracy:  0.546116869460718\n",
      "Epoch:  99 Val Accuracy:  0.44598337950138506\n",
      "Epoch:  0 Training Accuracy:  0.20129187321616343\n",
      "Epoch:  0 Val Accuracy:  0.21837488457987073\n",
      "Epoch:  1 Training Accuracy:  0.2238245455911071\n",
      "Epoch:  1 Val Accuracy:  0.2848568790397045\n",
      "Epoch:  2 Training Accuracy:  0.22712933753943218\n",
      "Epoch:  2 Val Accuracy:  0.2451523545706371\n",
      "Epoch:  3 Training Accuracy:  0.22487607030193782\n",
      "Epoch:  3 Val Accuracy:  0.3033240997229917\n",
      "Epoch:  4 Training Accuracy:  0.22262280306444343\n",
      "Epoch:  4 Val Accuracy:  0.28254847645429365\n",
      "Epoch:  5 Training Accuracy:  0.226002703920685\n",
      "Epoch:  5 Val Accuracy:  0.39335180055401664\n",
      "Epoch:  6 Training Accuracy:  0.265059336037254\n",
      "Epoch:  6 Val Accuracy:  0.4224376731301939\n",
      "Epoch:  7 Training Accuracy:  0.2426768814781433\n",
      "Epoch:  7 Val Accuracy:  0.40951061865189287\n",
      "Epoch:  8 Training Accuracy:  0.28308547393720895\n",
      "Epoch:  8 Val Accuracy:  0.4224376731301939\n",
      "Epoch:  9 Training Accuracy:  0.286540483701367\n",
      "Epoch:  9 Val Accuracy:  0.41458910433979684\n",
      "Epoch:  10 Training Accuracy:  0.3192128586450353\n",
      "Epoch:  10 Val Accuracy:  0.45060018467220686\n",
      "Epoch:  11 Training Accuracy:  0.3604476490911822\n",
      "Epoch:  11 Val Accuracy:  0.4695290858725762\n",
      "Epoch:  12 Training Accuracy:  0.3457263031395523\n",
      "Epoch:  12 Val Accuracy:  0.4598337950138504\n",
      "Epoch:  13 Training Accuracy:  0.39274447949526814\n",
      "Epoch:  13 Val Accuracy:  0.46121883656509693\n",
      "Epoch:  14 Training Accuracy:  0.41189725101397023\n",
      "Epoch:  14 Val Accuracy:  0.5203139427516159\n",
      "Epoch:  15 Training Accuracy:  0.46169445696259576\n",
      "Epoch:  15 Val Accuracy:  0.6274238227146814\n",
      "Epoch:  16 Training Accuracy:  0.5028541385008262\n",
      "Epoch:  16 Val Accuracy:  0.6168051708217913\n",
      "Epoch:  17 Training Accuracy:  0.5198287516899505\n",
      "Epoch:  17 Val Accuracy:  0.7049861495844876\n",
      "Epoch:  18 Training Accuracy:  0.5186270091632867\n",
      "Epoch:  18 Val Accuracy:  0.6777469990766389\n",
      "Epoch:  19 Training Accuracy:  0.5413850082619799\n",
      "Epoch:  19 Val Accuracy:  0.6819021237303786\n",
      "Epoch:  20 Training Accuracy:  0.5598617996094337\n",
      "Epoch:  20 Val Accuracy:  0.7160664819944599\n",
      "Epoch:  21 Training Accuracy:  0.5664713835060838\n",
      "Epoch:  21 Val Accuracy:  0.7317636195752539\n",
      "Epoch:  22 Training Accuracy:  0.5703770467177407\n",
      "Epoch:  22 Val Accuracy:  0.7040627885503232\n",
      "Epoch:  23 Training Accuracy:  0.5687246507435781\n",
      "Epoch:  23 Val Accuracy:  0.716528162511542\n",
      "Epoch:  24 Training Accuracy:  0.5743578188373141\n",
      "Epoch:  24 Val Accuracy:  0.7322253000923361\n",
      "Epoch:  25 Training Accuracy:  0.5793901156677181\n",
      "Epoch:  25 Val Accuracy:  0.7299168975069252\n",
      "Epoch:  26 Training Accuracy:  0.5863752441039507\n",
      "Epoch:  26 Val Accuracy:  0.7262234533702677\n",
      "Epoch:  27 Training Accuracy:  0.5807420760102148\n",
      "Epoch:  27 Val Accuracy:  0.7349953831948292\n",
      "Epoch:  28 Training Accuracy:  0.5856992639327024\n",
      "Epoch:  28 Val Accuracy:  0.7271468144044322\n",
      "Epoch:  29 Training Accuracy:  0.5917079765660207\n",
      "Epoch:  29 Val Accuracy:  0.7386888273314867\n",
      "Epoch:  30 Training Accuracy:  0.6025236593059937\n",
      "Epoch:  30 Val Accuracy:  0.7234533702677747\n",
      "Epoch:  31 Training Accuracy:  0.606804867057233\n",
      "Epoch:  31 Val Accuracy:  0.7072945521698984\n",
      "Epoch:  32 Training Accuracy:  0.6097341144659757\n",
      "Epoch:  32 Val Accuracy:  0.703601108033241\n",
      "Epoch:  33 Training Accuracy:  0.6143908667567973\n",
      "Epoch:  33 Val Accuracy:  0.7183748845798708\n",
      "Epoch:  34 Training Accuracy:  0.6131891242301337\n",
      "Epoch:  34 Val Accuracy:  0.7119113573407202\n",
      "Epoch:  35 Training Accuracy:  0.6137899954934656\n",
      "Epoch:  35 Val Accuracy:  0.7040627885503232\n",
      "Epoch:  36 Training Accuracy:  0.6170947874417906\n",
      "Epoch:  36 Val Accuracy:  0.7082179132040628\n",
      "Epoch:  37 Training Accuracy:  0.6195733814030344\n",
      "Epoch:  37 Val Accuracy:  0.7257617728531855\n",
      "Epoch:  38 Training Accuracy:  0.623103500075109\n",
      "Epoch:  38 Val Accuracy:  0.7262234533702677\n",
      "Epoch:  39 Training Accuracy:  0.6259576385759351\n",
      "Epoch:  39 Val Accuracy:  0.6897506925207756\n",
      "Epoch:  40 Training Accuracy:  0.6401532221721496\n",
      "Epoch:  40 Val Accuracy:  0.7119113573407202\n",
      "Epoch:  41 Training Accuracy:  0.6360222322367433\n",
      "Epoch:  41 Val Accuracy:  0.7248384118190212\n",
      "Epoch:  42 Training Accuracy:  0.645561063542136\n",
      "Epoch:  42 Val Accuracy:  0.698522622345337\n",
      "Epoch:  43 Training Accuracy:  0.6456361724500526\n",
      "Epoch:  43 Val Accuracy:  0.7109879963065558\n",
      "Epoch:  44 Training Accuracy:  0.6485654198587952\n",
      "Epoch:  44 Val Accuracy:  0.6975992613111727\n",
      "Epoch:  45 Training Accuracy:  0.6469130238846327\n",
      "Epoch:  45 Val Accuracy:  0.7123730378578024\n",
      "Epoch:  46 Training Accuracy:  0.6490911822142106\n",
      "Epoch:  46 Val Accuracy:  0.7068328716528163\n",
      "Epoch:  47 Training Accuracy:  0.6487156376746283\n",
      "Epoch:  47 Val Accuracy:  0.6962142197599261\n",
      "Epoch:  48 Training Accuracy:  0.6505933603725402\n",
      "Epoch:  48 Val Accuracy:  0.7040627885503232\n",
      "Epoch:  49 Training Accuracy:  0.6521706474387863\n",
      "Epoch:  49 Val Accuracy:  0.7054478301015698\n",
      "Epoch:  50 Training Accuracy:  0.6528466276100345\n",
      "Epoch:  50 Val Accuracy:  0.7054478301015698\n",
      "Epoch:  51 Training Accuracy:  0.6622352410995944\n",
      "Epoch:  51 Val Accuracy:  0.7012927054478301\n",
      "Epoch:  52 Training Accuracy:  0.6605828451254319\n",
      "Epoch:  52 Val Accuracy:  0.7082179132040628\n",
      "Epoch:  53 Training Accuracy:  0.6587051224275199\n",
      "Epoch:  53 Val Accuracy:  0.7091412742382271\n",
      "Epoch:  54 Training Accuracy:  0.6641880727054229\n",
      "Epoch:  54 Val Accuracy:  0.6980609418282548\n",
      "Epoch:  55 Training Accuracy:  0.6605828451254319\n",
      "Epoch:  55 Val Accuracy:  0.7096029547553093\n",
      "Epoch:  56 Training Accuracy:  0.6611837163887637\n",
      "Epoch:  56 Val Accuracy:  0.7008310249307479\n",
      "Epoch:  57 Training Accuracy:  0.6586300135196034\n",
      "Epoch:  57 Val Accuracy:  0.7183748845798708\n",
      "Epoch:  58 Training Accuracy:  0.665765359771669\n",
      "Epoch:  58 Val Accuracy:  0.6920590951061866\n",
      "Epoch:  59 Training Accuracy:  0.6648640528766712\n",
      "Epoch:  59 Val Accuracy:  0.6897506925207756\n",
      "Epoch:  60 Training Accuracy:  0.6680937359170798\n",
      "Epoch:  60 Val Accuracy:  0.6962142197599261\n",
      "Epoch:  61 Training Accuracy:  0.6670422112062491\n",
      "Epoch:  61 Val Accuracy:  0.7049861495844876\n",
      "Epoch:  62 Training Accuracy:  0.6680937359170798\n",
      "Epoch:  62 Val Accuracy:  0.698522622345337\n",
      "Epoch:  63 Training Accuracy:  0.6698963497070752\n",
      "Epoch:  63 Val Accuracy:  0.6948291782086796\n",
      "Epoch:  64 Training Accuracy:  0.6715487456812378\n",
      "Epoch:  64 Val Accuracy:  0.6929824561403509\n",
      "Epoch:  65 Training Accuracy:  0.6700465675229083\n",
      "Epoch:  65 Val Accuracy:  0.6925207756232687\n",
      "Epoch:  66 Training Accuracy:  0.6701967853387412\n",
      "Epoch:  66 Val Accuracy:  0.6948291782086796\n",
      "Epoch:  67 Training Accuracy:  0.669070151719994\n",
      "Epoch:  67 Val Accuracy:  0.6952908587257618\n",
      "Epoch:  68 Training Accuracy:  0.6710229833258224\n",
      "Epoch:  68 Val Accuracy:  0.695752539242844\n",
      "Epoch:  69 Training Accuracy:  0.6744779930899805\n",
      "Epoch:  69 Val Accuracy:  0.6948291782086796\n",
      "Epoch:  70 Training Accuracy:  0.6738771218266486\n",
      "Epoch:  70 Val Accuracy:  0.6999076638965835\n",
      "Epoch:  71 Training Accuracy:  0.6720745080366531\n",
      "Epoch:  71 Val Accuracy:  0.6966759002770083\n",
      "Epoch:  72 Training Accuracy:  0.6709478744179059\n",
      "Epoch:  72 Val Accuracy:  0.6943674976915974\n",
      "Epoch:  73 Training Accuracy:  0.6733513594712333\n",
      "Epoch:  73 Val Accuracy:  0.6925207756232687\n",
      "Epoch:  74 Training Accuracy:  0.6729758149316509\n",
      "Epoch:  74 Val Accuracy:  0.6929824561403509\n",
      "Epoch:  75 Training Accuracy:  0.6753792999849783\n",
      "Epoch:  75 Val Accuracy:  0.6971375807940905\n",
      "Epoch:  76 Training Accuracy:  0.6740273396424816\n",
      "Epoch:  76 Val Accuracy:  0.6906740535549399\n",
      "Epoch:  77 Training Accuracy:  0.6739522307345651\n",
      "Epoch:  77 Val Accuracy:  0.6948291782086796\n",
      "Epoch:  78 Training Accuracy:  0.6744779930899805\n",
      "Epoch:  78 Val Accuracy:  0.6939058171745153\n",
      "Epoch:  79 Training Accuracy:  0.6767312603274749\n",
      "Epoch:  79 Val Accuracy:  0.7003693444136657\n",
      "Epoch:  80 Training Accuracy:  0.6755295178008112\n",
      "Epoch:  80 Val Accuracy:  0.6980609418282548\n",
      "Epoch:  81 Training Accuracy:  0.6755295178008112\n",
      "Epoch:  81 Val Accuracy:  0.7012927054478301\n",
      "Epoch:  82 Training Accuracy:  0.6774072404987231\n",
      "Epoch:  82 Val Accuracy:  0.6975992613111727\n",
      "Epoch:  83 Training Accuracy:  0.6779330028541385\n",
      "Epoch:  83 Val Accuracy:  0.6971375807940905\n",
      "Epoch:  84 Training Accuracy:  0.6772570226828902\n",
      "Epoch:  84 Val Accuracy:  0.695752539242844\n",
      "Epoch:  85 Training Accuracy:  0.6789845275649692\n",
      "Epoch:  85 Val Accuracy:  0.6980609418282548\n",
      "Epoch:  86 Training Accuracy:  0.677857893946222\n",
      "Epoch:  86 Val Accuracy:  0.6920590951061866\n",
      "Epoch:  87 Training Accuracy:  0.6787592008412198\n",
      "Epoch:  87 Val Accuracy:  0.6994459833795014\n",
      "Epoch:  88 Training Accuracy:  0.6775574583145562\n",
      "Epoch:  88 Val Accuracy:  0.7003693444136657\n",
      "Epoch:  89 Training Accuracy:  0.6775574583145562\n",
      "Epoch:  89 Val Accuracy:  0.6980609418282548\n",
      "Epoch:  90 Training Accuracy:  0.6789094186570527\n",
      "Epoch:  90 Val Accuracy:  0.6962142197599261\n",
      "Epoch:  91 Training Accuracy:  0.6783836563016373\n",
      "Epoch:  91 Val Accuracy:  0.7008310249307479\n",
      "Epoch:  92 Training Accuracy:  0.677857893946222\n",
      "Epoch:  92 Val Accuracy:  0.7022160664819944\n",
      "Epoch:  93 Training Accuracy:  0.6757548445245606\n",
      "Epoch:  93 Val Accuracy:  0.6975992613111727\n",
      "Epoch:  94 Training Accuracy:  0.678008111762055\n",
      "Epoch:  94 Val Accuracy:  0.698522622345337\n",
      "Epoch:  95 Training Accuracy:  0.6789845275649692\n",
      "Epoch:  95 Val Accuracy:  0.6994459833795014\n",
      "Epoch:  96 Training Accuracy:  0.679435181012468\n",
      "Epoch:  96 Val Accuracy:  0.698522622345337\n",
      "Epoch:  97 Training Accuracy:  0.6788343097491363\n",
      "Epoch:  97 Val Accuracy:  0.6989843028624192\n",
      "Epoch:  98 Training Accuracy:  0.6798107255520505\n",
      "Epoch:  98 Val Accuracy:  0.6999076638965835\n",
      "Epoch:  99 Training Accuracy:  0.677857893946222\n",
      "Epoch:  99 Val Accuracy:  0.7003693444136657\n",
      "Epoch:  0 Training Accuracy:  0.2501126633618747\n",
      "Epoch:  0 Val Accuracy:  0.3808864265927978\n",
      "Epoch:  1 Training Accuracy:  0.2611536728255971\n",
      "Epoch:  1 Val Accuracy:  0.3471837488457987\n",
      "Epoch:  2 Training Accuracy:  0.28195884031846175\n",
      "Epoch:  2 Val Accuracy:  0.38827331486611266\n",
      "Epoch:  3 Training Accuracy:  0.2547694156526964\n",
      "Epoch:  3 Val Accuracy:  0.2765466297322253\n",
      "Epoch:  4 Training Accuracy:  0.3245455911071053\n",
      "Epoch:  4 Val Accuracy:  0.3716528162511542\n",
      "Epoch:  5 Training Accuracy:  0.3423464022833108\n",
      "Epoch:  5 Val Accuracy:  0.3554939981532779\n",
      "Epoch:  6 Training Accuracy:  0.3525612137599519\n",
      "Epoch:  6 Val Accuracy:  0.32871652816251157\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m y_pred\u001b[38;5;241m=\u001b[39mmodel(x)\n\u001b[1;32m     22\u001b[0m loss\u001b[38;5;241m=\u001b[39mloss_fn(y_pred,y)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m loss_sum\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_classes=6\n",
    "for dropout in [0.1]:\n",
    "    for layer in [3,2,4]:\n",
    "            for hidden_dim in [10,16,32,64]:\n",
    "                if layer==3 and hidden_dim!=64:\n",
    "                    continue\n",
    "                torch.manual_seed(0)\n",
    "                model=LSTMModel(10,hidden_dim,layer,output_classes,dropout)\n",
    "                optimizer=torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "                scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=20,gamma=0.3)\n",
    "                if not os.path.exists('model6_3s/0.01_'+str(dropout)+'_'+str(layer)+'_'+str(hidden_dim)):\n",
    "                    os.makedirs('model6_3s/0.01_'+str(dropout)+'_'+str(layer)+'_'+str(hidden_dim))\n",
    "                filename='model6_3s/0.01_'+str(dropout)+'_'+str(layer)+'_'+str(hidden_dim)\n",
    "                f=open(filename+'/results.txt','w')\n",
    "                loss_arr=[]\n",
    "                training_accuracy=[]\n",
    "                val_accuracy=[]\n",
    "                for epoch in range(100):\n",
    "                    model.train()\n",
    "                    loss_sum=0\n",
    "                    for i,(x,y) in enumerate(train):\n",
    "                        optimizer.zero_grad()\n",
    "                        y_pred=model(x)\n",
    "                        loss=loss_fn(y_pred,y)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        loss_sum+=loss.item()\n",
    "                    scheduler.step()\n",
    "                    loss_sum/=len(train)\n",
    "                    loss_arr.append(loss_sum)\n",
    "                    model.eval()\n",
    "                    correct=0\n",
    "                    total=0\n",
    "                    #Calculate accuracy per class \n",
    "                    accuracy_per_class=[0]*output_classes\n",
    "                    fp_per_class=[0]*output_classes\n",
    "                    fn_per_class=[0]*output_classes\n",
    "                    tp_per_class=[0]*output_classes\n",
    "                    tn_per_class=[0]*output_classes\n",
    "                    fp_rate_per_class=[0]*output_classes\n",
    "                    for i,(x,y) in enumerate(train):\n",
    "                        y_pred=model(x)\n",
    "                        _,predicted=torch.max(y_pred.data,1)\n",
    "                        total+=y.size(0)\n",
    "                        y=torch.max(y,1)[1]\n",
    "                        correct+=(predicted==y).sum().item()\n",
    "                        for i in range(len(y)):\n",
    "                            if y[i]==predicted[i]:\n",
    "                                tp_per_class[y[i]]+=1\n",
    "                                for j in range(output_classes):\n",
    "                                    if j!=y[i]:\n",
    "                                        tn_per_class[j]+=1\n",
    "                            else:\n",
    "                                fp_per_class[predicted[i]]+=1\n",
    "                                fn_per_class[y[i]]+=1\n",
    "                                for j in range(output_classes):\n",
    "                                    if j!=y[i] and j!=predicted[i]:\n",
    "                                        tn_per_class[j]+=1\n",
    "                    for i in range(output_classes):\n",
    "                        fp_rate_per_class[i]=fp_per_class[i]/(fp_per_class[i]+tn_per_class[i])\n",
    "                        accuracy_per_class[i]=tp_per_class[i]/(tp_per_class[i]+fn_per_class[i])\n",
    "                    print('Epoch: ',epoch,'Training Accuracy: ',correct/total)\n",
    "                    training_accuracy.append(correct/total)\n",
    "                    f.write('Epoch: '+str(epoch)+' Training Accuracy: '+str(correct/total)+'\\n')\n",
    "                    f.write('Accuracy per class: '+str(accuracy_per_class)+'\\n')\n",
    "                    f.write('False positive rate per class: '+str(fp_rate_per_class)+'\\n')\n",
    "                    f.write('False positive per class: '+str(fp_per_class)+'\\n')\n",
    "                    f.write('False negative per class: '+str(fn_per_class)+'\\n')\n",
    "                    f.write('True positive per class: '+str(tp_per_class)+'\\n')\n",
    "                    f.write('True negative per class: '+str(tn_per_class)+'\\n')\n",
    "                    f.write('\\n')\n",
    "                    correct=0\n",
    "                    total=0\n",
    "                    accuracy_per_class=[0]*output_classes\n",
    "                    fp_per_class=[0]*output_classes\n",
    "                    fn_per_class=[0]*output_classes\n",
    "                    tp_per_class=[0]*output_classes\n",
    "                    tn_per_class=[0]*output_classes\n",
    "                    fp_rate_per_class=[0]*output_classes\n",
    "                    for i,(x,y) in enumerate(val):\n",
    "                        y_pred=model(x)\n",
    "                        _,predicted=torch.max(y_pred.data,1)\n",
    "                        total+=y.size(0)\n",
    "                        y=torch.max(y,1)[1]\n",
    "                        correct+=(predicted==y).sum().item()\n",
    "                        for i in range(len(y)):\n",
    "                            if y[i]==predicted[i]:\n",
    "                                tp_per_class[y[i]]+=1\n",
    "                                for j in range(output_classes):\n",
    "                                    if j!=y[i]:\n",
    "                                        tn_per_class[j]+=1\n",
    "                            else:\n",
    "                                fp_per_class[predicted[i]]+=1\n",
    "                                fn_per_class[y[i]]+=1\n",
    "                                for j in range(output_classes):\n",
    "                                    if j!=y[i] and j!=predicted[i]:\n",
    "                                        tn_per_class[j]+=1\n",
    "                    for i in range(output_classes):\n",
    "                        fp_rate_per_class[i]=fp_per_class[i]/(fp_per_class[i]+tn_per_class[i])\n",
    "                        if tp_per_class[i]+fn_per_class[i]==0:\n",
    "                            accuracy_per_class[i]=1\n",
    "                        else:\n",
    "                            accuracy_per_class[i]=tp_per_class[i]/(tp_per_class[i]+fn_per_class[i])\n",
    "                    print('Epoch: ',epoch,'Val Accuracy: ',correct/total)\n",
    "                    val_accuracy.append(correct/total)\n",
    "                    f.write('Epoch: '+str(epoch)+' Val Accuracy: '+str(correct/total)+'\\n')\n",
    "                    f.write('Accuracy per class: '+str(accuracy_per_class)+'\\n')\n",
    "                    f.write('False positive rate per class: '+str(fp_rate_per_class)+'\\n')\n",
    "                    f.write('False positive per class: '+str(fp_per_class)+'\\n')\n",
    "                    f.write('False negative per class: '+str(fn_per_class)+'\\n')\n",
    "                    f.write('True positive per class: '+str(tp_per_class)+'\\n')\n",
    "                    f.write('True negative per class: '+str(tn_per_class)+'\\n')\n",
    "                    f.write('\\n')\n",
    "                    torch.save(model,filename+'/model'+str(epoch)+'.pt')   \n",
    "                plt.plot(loss_arr)\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.savefig(filename+'/loss.png')\n",
    "                plt.close()\n",
    "                plt.plot(training_accuracy)\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Training Accuracy')\n",
    "                plt.savefig(filename+'/training_accuracy.png')\n",
    "                plt.close()\n",
    "                plt.plot(val_accuracy)\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Validation Accuracy')\n",
    "                plt.savefig(filename+'/val_accuracy.png')\n",
    "                f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
